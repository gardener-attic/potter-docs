[
{
	"uri": "https://gardener.github.io/potter-docs/controller-docs-0.112.0/home/",
	"title": "Home",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gardener.github.io/potter-docs/controller-docs-0.113.0/home/",
	"title": "Home",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gardener.github.io/potter-docs/controller-docs/home/",
	"title": "Home",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gardener.github.io/potter-docs/hub-docs-0.112.0/home/",
	"title": "Home",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gardener.github.io/potter-docs/hub-docs-0.113.0/home/",
	"title": "Home",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gardener.github.io/potter-docs/hub-docs/home/",
	"title": "Home",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gardener.github.io/potter-docs/controller-docs-0.112.0/docs/helm-example/",
	"title": "Cluster-BoM Example with Helm Applications",
	"tags": [],
	"description": "",
	"content": "In this example two Helm applications are deployed, one from an incubator Helm Chart repository and one using a direct chart link. This example uses the project name apphubdemo in Gardener and the shoot cluster named my-cluster.\napiVersion:\u0026#34;hub.k8s.sap.com/v1\u0026#34;kind:ClusterBommetadata:name:demo # Cluster-BoM name.namespace: garden-apphubdemo # Cluster-BoM namespace. Pattern:garden-\u0026lt;projectname in Gardener\u0026gt;spec:secretRef:my-cluster.kubeconfig # Reference to kubeconfig of target cluster # Pattern: \u0026lt;name of Kubernetes cluster in gardener\u0026gt;.kubeconfigautoDelete:# Auto delete of Cluster-BoM (optional)clusterBomAge:120# Custer-BoM age in minutes# Cluster-BoM is deleted automatically if the# corresponding shoot cluster is removed and the # Clustere-BoM is older than the specified clusterBomAge. applicationConfigs:# List of applications to be deployed in target cluster- id:karydia # Unique id for application for this BoM-ClusternoReconcile:true# Exclude application from reconcilation loop # (optional with default false)readyRequirements:# (optional)jobs:# (optional) Jobs which must succeed as a precondition for a - name:testJob1 # successfully deployed and running applicationnamespace:testNamespaceconfigType:helm # Helm deployment, more types are plannedvalues:# The value section, Helm chart values in this exampleconfig:cloudProvider:\u0026#34;GCP\u0026#34;# Helm chart specific value typeSpecificData:# Details about what exactly to deployinstallName:\u0026#34;securityconfig\u0026#34;# Name of the deployment (arbitrary)namespace:\u0026#34;karydia\u0026#34;# Namespace in target cluster where to install the applicationinstallTimeout:10# Timeout in minutes for the Helm install command (optional default=5)upgradeTimeout:10# Timeout in minutes for the Helm upgrade command (optional default=5)uninstallTimeout:10# Timeout in minutes for the Helm uninstall command (optional default=5)installArguments:# (optional) Arguments used for helm install.- atomic # Currently only atomic is supported. updateArguments:# (optional) Arguments used for helm upgrade.- atomic # Currently only atomic is supported. catalogAccess:# Catalog specification where the Helm chart can be found:chartName:\u0026#34;karydia\u0026#34;# Name of the Helm chartrepo:\u0026#34;incubator\u0026#34;# Name of the Helm chart repositorychartVersion:\u0026#34;0.3.1\u0026#34;# Helm chart version to be deployed- id:mongodb # The second application within this Cluster-BoMconfigType:helmtypeSpecificData:# Helm settings (see above)installName:\u0026#34;mongodb\u0026#34;namespace:\u0026#34;default\u0026#34;tarballAccess:# Allows specifying an url pointing to the packaged charturl:\u0026#34;https://charts.bitnami.com/bitnami/mongodb-7.8.4.tgz\u0026#34;customCAData:# (optional) This property allows you to add a custom CA, # which is useful if your server speaks HTTPS with a self-# signed certificate. The added certificate must be # in PEM format and base64 encoded.authHeader:Basic dX...3dvcmQ= # (optional) The value of this property will be set # in the \u0026#34;Authorization\u0026#34; header when fetching the Chart. # For Basic Authentication, the credentials can also # be stored as part of the url instead of using the# \u0026#34;authHeader\u0026#34; property, for example, # https://username:password@example.com/my-chart.tgz.secretRef:# (optional) Reference to a secret in the same namespace as the name:someSecretName # clusterbom, containing an entry with key \u0026#34;authHeader\u0026#34;. # The value of this entry will be set in the \u0026#34;Authorization\u0026#34; # header when fetching the Chart (e.g. \u0026#34;Basic dX...3dvcmQ=\u0026#34;). # You could also reference a named secret here (see # https://gardener.github.io/potter-docs/controller-docs/docs/special-topics/named-secrets/).# more application deployments can go here"
},
{
	"uri": "https://gardener.github.io/potter-docs/controller-docs-0.113.0/docs/helm-example/",
	"title": "Cluster-BoM Example with Helm Applications",
	"tags": [],
	"description": "",
	"content": "In this example two Helm applications are deployed, one from an incubator Helm Chart repository and one using a direct chart link. This example uses the project name apphubdemo in Gardener and the shoot cluster named my-cluster.\napiVersion:\u0026#34;hub.k8s.sap.com/v1\u0026#34;kind:ClusterBommetadata:name:demo # Cluster-BoM name.namespace: garden-apphubdemo # Cluster-BoM namespace. Pattern:garden-\u0026lt;projectname in Gardener\u0026gt;spec:secretRef:my-cluster.kubeconfig # Reference to kubeconfig of target cluster # Pattern: \u0026lt;name of Kubernetes cluster in gardener\u0026gt;.kubeconfigautoDelete:# Auto delete of Cluster-BoM (optional)clusterBomAge:120# Custer-BoM age in minutes# Cluster-BoM is deleted automatically if the# corresponding shoot cluster is removed and the # Clustere-BoM is older than the specified clusterBomAge. applicationConfigs:# List of applications to be deployed in target cluster- id:karydia # Unique id for application for this BoM-ClusternoReconcile:true# Exclude application from reconcilation loop # (optional with default false)readyRequirements:# (optional)jobs:# (optional) Jobs which must succeed as a precondition for a - name:testJob1 # successfully deployed and running applicationnamespace:testNamespaceconfigType:helm # Helm deployment, more types are plannedvalues:# The value section, Helm chart values in this exampleconfig:cloudProvider:\u0026#34;GCP\u0026#34;# Helm chart specific value typeSpecificData:# Details about what exactly to deployinstallName:\u0026#34;securityconfig\u0026#34;# Name of the deployment (arbitrary)namespace:\u0026#34;karydia\u0026#34;# Namespace in target cluster where to install the applicationinstallTimeout:10# Timeout in minutes for the Helm install command (optional default=5)upgradeTimeout:10# Timeout in minutes for the Helm upgrade command (optional default=5)uninstallTimeout:10# Timeout in minutes for the Helm uninstall command (optional default=5)installArguments:# (optional) Arguments used for helm install.- atomic # Currently only atomic is supported. updateArguments:# (optional) Arguments used for helm upgrade.- atomic # Currently only atomic is supported. catalogAccess:# Catalog specification where the Helm chart can be found:chartName:\u0026#34;karydia\u0026#34;# Name of the Helm chartrepo:\u0026#34;incubator\u0026#34;# Name of the Helm chart repositorychartVersion:\u0026#34;0.3.1\u0026#34;# Helm chart version to be deployed- id:mongodb # The second application within this Cluster-BoMconfigType:helmtypeSpecificData:# Helm settings (see above)installName:\u0026#34;mongodb\u0026#34;namespace:\u0026#34;default\u0026#34;tarballAccess:# Allows specifying an url pointing to the packaged charturl:\u0026#34;https://charts.bitnami.com/bitnami/mongodb-7.8.4.tgz\u0026#34;customCAData:# (optional) This property allows you to add a custom CA, # which is useful if your server speaks HTTPS with a self-# signed certificate. The added certificate must be # in PEM format and base64 encoded.authHeader:Basic dX...3dvcmQ= # (optional) The value of this property will be set # in the \u0026#34;Authorization\u0026#34; header when fetching the Chart. # For Basic Authentication, the credentials can also # be stored as part of the url instead of using the# \u0026#34;authHeader\u0026#34; property, for example, # https://username:password@example.com/my-chart.tgz.secretRef:# (optional) Reference to a secret in the same namespace as the name:someSecretName # clusterbom, containing an entry with key \u0026#34;authHeader\u0026#34;. # The value of this entry will be set in the \u0026#34;Authorization\u0026#34; # header when fetching the Chart (e.g. \u0026#34;Basic dX...3dvcmQ=\u0026#34;). # You could also reference a named secret here (see # https://gardener.github.io/potter-docs/controller-docs/docs/special-topics/named-secrets/).# more application deployments can go here"
},
{
	"uri": "https://gardener.github.io/potter-docs/controller-docs/docs/helm-example/",
	"title": "Cluster-BoM Example with Helm Applications",
	"tags": [],
	"description": "",
	"content": "In this example two Helm applications are deployed, one from an incubator Helm Chart repository and one using a direct chart link. This example uses the project name apphubdemo in Gardener and the shoot cluster named my-cluster.\napiVersion:\u0026#34;hub.k8s.sap.com/v1\u0026#34;kind:ClusterBommetadata:name:demo # Cluster-BoM name.namespace: garden-apphubdemo # Cluster-BoM namespace. Pattern:garden-\u0026lt;projectname in Gardener\u0026gt;spec:secretRef:my-cluster.kubeconfig # Reference to kubeconfig of target cluster # Pattern: \u0026lt;name of Kubernetes cluster in gardener\u0026gt;.kubeconfigautoDelete:# Auto delete of Cluster-BoM (optional)clusterBomAge:120# Custer-BoM age in minutes# Cluster-BoM is deleted automatically if the# corresponding shoot cluster is removed and the # Clustere-BoM is older than the specified clusterBomAge. applicationConfigs:# List of applications to be deployed in target cluster- id:karydia # Unique id for application for this BoM-ClusternoReconcile:true# Exclude application from reconcilation loop # (optional with default false)readyRequirements:# (optional)jobs:# (optional) Jobs which must succeed as a precondition for a - name:testJob1 # successfully deployed and running applicationnamespace:testNamespaceconfigType:helm # Helm deployment, more types are plannedvalues:# The value section, Helm chart values in this exampleconfig:cloudProvider:\u0026#34;GCP\u0026#34;# Helm chart specific value typeSpecificData:# Details about what exactly to deployinstallName:\u0026#34;securityconfig\u0026#34;# Name of the deployment (arbitrary)namespace:\u0026#34;karydia\u0026#34;# Namespace in target cluster where to install the applicationinstallTimeout:10# Timeout in minutes for the Helm install command (optional default=5)upgradeTimeout:10# Timeout in minutes for the Helm upgrade command (optional default=5)uninstallTimeout:10# Timeout in minutes for the Helm uninstall command (optional default=5)installArguments:# (optional) Arguments used for helm install.- atomic # Currently only atomic is supported. updateArguments:# (optional) Arguments used for helm upgrade.- atomic # Currently only atomic is supported. catalogAccess:# Catalog specification where the Helm chart can be found:chartName:\u0026#34;karydia\u0026#34;# Name of the Helm chartrepo:\u0026#34;incubator\u0026#34;# Name of the Helm chart repositorychartVersion:\u0026#34;0.3.1\u0026#34;# Helm chart version to be deployed- id:mongodb # The second application within this Cluster-BoMconfigType:helmtypeSpecificData:# Helm settings (see above)installName:\u0026#34;mongodb\u0026#34;namespace:\u0026#34;default\u0026#34;tarballAccess:# Allows specifying an url pointing to the packaged charturl:\u0026#34;https://charts.bitnami.com/bitnami/mongodb-7.8.4.tgz\u0026#34;customCAData:# (optional) This property allows you to add a custom CA, # which is useful if your server speaks HTTPS with a self-# signed certificate. The added certificate must be # in PEM format and base64 encoded.authHeader:Basic dX...3dvcmQ= # (optional) The value of this property will be set # in the \u0026#34;Authorization\u0026#34; header when fetching the Chart. # For Basic Authentication, the credentials can also # be stored as part of the url instead of using the# \u0026#34;authHeader\u0026#34; property, for example, # https://username:password@example.com/my-chart.tgz.secretRef:# (optional) Reference to a secret in the same namespace as the name:someSecretName # clusterbom, containing an entry with key \u0026#34;authHeader\u0026#34;. # The value of this entry will be set in the \u0026#34;Authorization\u0026#34; # header when fetching the Chart (e.g. \u0026#34;Basic dX...3dvcmQ=\u0026#34;). # You could also reference a named secret here (see # https://gardener.github.io/potter-docs/controller-docs/docs/special-topics/named-secrets/).# more application deployments can go here"
},
{
	"uri": "https://gardener.github.io/potter-docs/controller-docs-0.112.0/docs/kapp-example/",
	"title": "Cluster-BoM Example with kapp Applications",
	"tags": [],
	"description": "",
	"content": "In this example, kapp applications are deployed. For kapp deployments the kapp controller is used, providing a large variety of different fetch, and template mechanisms. Therefore the format of typeSpecificData is similar to the specification used there.\nThe section cluster.kubeconfigSecretRef is optional and if missing secretRef of the Cluster-BoM is used for cluster.kubeconfigSecretRef.name with key kubeconfig. If cluster.kubeconfigSecretRef.name is specified it must be equal to secretRef of the Cluster-BoM.\nOf course it is also possible to add more than one kapp application to a Cluster-BoM or mix it with Helm chart deployments.\nThe currently deployed version of the kapp controller could be found under kappImage.tag in the values file.\napiVersion:\u0026#34;hub.k8s.sap.com/v1\u0026#34;kind:ClusterBommetadata:name:demo # Cluster-BoM name.namespace: garden-apphubdemo # Cluster-BoM namespace. Pattern:garden-\u0026lt;projectname in Gardener\u0026gt;spec:secretRef:my-cluster.kubeconfig # Reference to kubeconfig of target cluster # Pattern: \u0026lt;name of Kubernetes cluster in gardener\u0026gt;.kubeconfigautoDelete:# Auto delete of Cluster-BoM (optional)clusterBomAge:120# Custer-BoM age in minutes# Cluster-BoM is deleted automatically if the# corresponding shoot cluster is removed and the # Clustere-BoM is older than the specified clusterBomAge. applicationConfigs:# List of applications to be deployed in target cluster- id:kappexample1 # ID of the application within this Cluster-BoMconfigType:kapp # Deployment via kapp controller typeSpecificData:# Spec of a KAPP App, see# https://github.com/k14s/kapp-controller/blob/develop/docs/app-spec.mdfetch:- git:ref:origin/developurl:https://github.com/k14s/k8s-simple-app-exampletemplate:- ytt:paths:- config-step-2-templatedeploy:- kapp:{}- id:kappexample2 configType:kapp typeSpecificData:fetch:- http:url:https://storage.googleapis.com/hub-tarballs/kapp/success/http-zip-yml/config/simple.ziptemplate:- ytt:{}deploy:- kapp:intoNs:testnamespace# more application deployments can go hereTypical pitfalls we found so far with the specification of the fetch section using http (might be resolved in future versions of the kapp controller):\n Yaml files referenced via http fetch must be packaged e.g. in a zip file. Be careful if you create zip files on a Mac. A folder __MAXOSX is added to the archive automatically and must be removed before usage e.g. via the command  zip -d example.zip \u0026#34;__MAXOSX*\u0026#34;`.  The yaml files in the archive must have the file ending yml and not yaml.  For kapp applications, in the type specific data of the Cluster-BoM, you have the possibility to reference secrets via secretRef entries. The secrets must be stored in the same cluster and namespace as the corresponding Cluster-BoM. More details can be found here.\nCurrently the values section of application configs is not considered for kapp deployments.\n"
},
{
	"uri": "https://gardener.github.io/potter-docs/controller-docs-0.113.0/docs/kapp-example/",
	"title": "Cluster-BoM Example with kapp Applications",
	"tags": [],
	"description": "",
	"content": "In this example, kapp applications are deployed. For kapp deployments the kapp controller is used, providing a large variety of different fetch, and template mechanisms. Therefore the format of typeSpecificData is similar to the specification used there.\nThe section cluster.kubeconfigSecretRef is optional and if missing secretRef of the Cluster-BoM is used for cluster.kubeconfigSecretRef.name with key kubeconfig. If cluster.kubeconfigSecretRef.name is specified it must be equal to secretRef of the Cluster-BoM.\nOf course it is also possible to add more than one kapp application to a Cluster-BoM or mix it with Helm chart deployments.\nThe currently deployed version of the kapp controller could be found under kappImage.tag in the values file.\napiVersion:\u0026#34;hub.k8s.sap.com/v1\u0026#34;kind:ClusterBommetadata:name:demo # Cluster-BoM name.namespace: garden-apphubdemo # Cluster-BoM namespace. Pattern:garden-\u0026lt;projectname in Gardener\u0026gt;spec:secretRef:my-cluster.kubeconfig # Reference to kubeconfig of target cluster # Pattern: \u0026lt;name of Kubernetes cluster in gardener\u0026gt;.kubeconfigautoDelete:# Auto delete of Cluster-BoM (optional)clusterBomAge:120# Custer-BoM age in minutes# Cluster-BoM is deleted automatically if the# corresponding shoot cluster is removed and the # Clustere-BoM is older than the specified clusterBomAge. applicationConfigs:# List of applications to be deployed in target cluster- id:kappexample1 # ID of the application within this Cluster-BoMconfigType:kapp # Deployment via kapp controller typeSpecificData:# Spec of a KAPP App, see# https://github.com/k14s/kapp-controller/blob/develop/docs/app-spec.mdfetch:- git:ref:origin/developurl:https://github.com/k14s/k8s-simple-app-exampletemplate:- ytt:paths:- config-step-2-templatedeploy:- kapp:{}- id:kappexample2 configType:kapp typeSpecificData:fetch:- http:url:https://storage.googleapis.com/hub-tarballs/kapp/success/http-zip-yml/config/simple.ziptemplate:- ytt:{}deploy:- kapp:intoNs:testnamespace# more application deployments can go hereTypical pitfalls we found so far with the specification of the fetch section using http (might be resolved in future versions of the kapp controller):\n Yaml files referenced via http fetch must be packaged e.g. in a zip file. Be careful if you create zip files on a Mac. A folder __MAXOSX is added to the archive automatically and must be removed before usage e.g. via the command  zip -d example.zip \u0026#34;__MAXOSX*\u0026#34;`.  The yaml files in the archive must have the file ending yml and not yaml.  For kapp applications, in the type specific data of the Cluster-BoM, you have the possibility to reference secrets via secretRef entries. The secrets must be stored in the same cluster and namespace as the corresponding Cluster-BoM. More details can be found here.\nCurrently the values section of application configs is not considered for kapp deployments.\n"
},
{
	"uri": "https://gardener.github.io/potter-docs/controller-docs/docs/kapp-example/",
	"title": "Cluster-BoM Example with kapp Applications",
	"tags": [],
	"description": "",
	"content": "In this example, kapp applications are deployed. For kapp deployments the kapp controller is used, providing a large variety of different fetch, and template mechanisms. Therefore the format of typeSpecificData is similar to the specification used there.\nThe section cluster.kubeconfigSecretRef is optional and if missing secretRef of the Cluster-BoM is used for cluster.kubeconfigSecretRef.name with key kubeconfig. If cluster.kubeconfigSecretRef.name is specified it must be equal to secretRef of the Cluster-BoM.\nOf course it is also possible to add more than one kapp application to a Cluster-BoM or mix it with Helm chart deployments.\nThe currently deployed version of the kapp controller could be found under kappImage.tag in the values file.\napiVersion:\u0026#34;hub.k8s.sap.com/v1\u0026#34;kind:ClusterBommetadata:name:demo # Cluster-BoM name.namespace: garden-apphubdemo # Cluster-BoM namespace. Pattern:garden-\u0026lt;projectname in Gardener\u0026gt;spec:secretRef:my-cluster.kubeconfig # Reference to kubeconfig of target cluster # Pattern: \u0026lt;name of Kubernetes cluster in gardener\u0026gt;.kubeconfigautoDelete:# Auto delete of Cluster-BoM (optional)clusterBomAge:120# Custer-BoM age in minutes# Cluster-BoM is deleted automatically if the# corresponding shoot cluster is removed and the # Clustere-BoM is older than the specified clusterBomAge. applicationConfigs:# List of applications to be deployed in target cluster- id:kappexample1 # ID of the application within this Cluster-BoMconfigType:kapp # Deployment via kapp controller typeSpecificData:# Spec of a KAPP App, see# https://github.com/k14s/kapp-controller/blob/develop/docs/app-spec.mdfetch:- git:ref:origin/developurl:https://github.com/k14s/k8s-simple-app-exampletemplate:- ytt:paths:- config-step-2-templatedeploy:- kapp:{}- id:kappexample2 configType:kapp typeSpecificData:fetch:- http:url:https://storage.googleapis.com/hub-tarballs/kapp/success/http-zip-yml/config/simple.ziptemplate:- ytt:{}deploy:- kapp:intoNs:testnamespace# more application deployments can go hereTypical pitfalls we found so far with the specification of the fetch section using http (might be resolved in future versions of the kapp controller):\n Yaml files referenced via http fetch must be packaged e.g. in a zip file. Be careful if you create zip files on a Mac. A folder __MAXOSX is added to the archive automatically and must be removed before usage e.g. via the command  zip -d example.zip \u0026#34;__MAXOSX*\u0026#34;`.  The yaml files in the archive must have the file ending yml and not yaml.  For kapp applications, in the type specific data of the Cluster-BoM, you have the possibility to reference secrets via secretRef entries. The secrets must be stored in the same cluster and namespace as the corresponding Cluster-BoM. More details can be found here.\nCurrently the values section of application configs is not considered for kapp deployments.\n"
},
{
	"uri": "https://gardener.github.io/potter-docs/controller-docs-0.112.0/docs/plain-yaml-example/",
	"title": "Cluster-BoM Example with plain yaml K8s resources",
	"tags": [],
	"description": "",
	"content": "In this example, kubernetes resources specified by plain yaml files are deployed. This is a special case of kapp deployments described here.\nThe provided kubernetes yaml resources are deployed according to the rules specified here, e.g. CRDs and namspaces are deployed before namespaced resources.\nThe first example uses a zip file containing one yml file with different kubernetes resources, which is accessible via the URL https://storage.googleapis.com/hub-tarballs/integration-tests/zip-http-plain-yml/config.zip.\nOf course the zip might contain more than one yml file. For pitfalls with respect to the zip creation see the kapp example.\napiVersion:\u0026#34;hub.k8s.sap.com/v1\u0026#34;kind:ClusterBommetadata:name:demo # Cluster-BoM name.namespace: garden-apphubdemo # Cluster-BoM namespace. Pattern:garden-\u0026lt;projectname in Gardener\u0026gt;spec:secretRef:my-cluster.kubeconfig # Reference to kubeconfig of target cluster # Pattern: \u0026lt;name of Kubernetes cluster in gardener\u0026gt;.kubeconfigautoDelete:# Auto delete of Cluster-BoM (optional)clusterBomAge:120# Custer-BoM age in minutes# Cluster-BoM is deleted automatically if the# corresponding shoot cluster is removed and the # Clustere-BoM is older than the specified clusterBomAge. applicationConfigs:# List of applications to be deployed in target cluster- id:yamlexample1 # ID of the application within this Cluster-BoMconfigType:kapp # Deployment via kapp controller typeSpecificData:# Spec of a YAML/KAPP App, see# https://github.com/k14s/kapp-controller/blob/develop/docs/app-spec.mdfetch:- http:url:https://storage.googleapis.com/hub-tarballs/integration-tests/zip-http-plain-yml/config.ziptemplate:# No templating- ytt:{}deploy:- kapp:{}# more application deployments can go hereThe second example fetches the yml files from a git repository. Of course the specified folder in the Git repository might contain more than one yml file.\napiVersion:\u0026#34;hub.k8s.sap.com/v1\u0026#34;kind:ClusterBommetadata:name:demo # Cluster-BoM name.namespace: garden-apphubdemo # Cluster-BoM namespace. Pattern:garden-\u0026lt;projectname in Gardener\u0026gt;spec:secretRef:my-cluster.kubeconfig # Reference to kubeconfig of target cluster # Pattern: \u0026lt;name of Kubernetes cluster in gardener\u0026gt;.kubeconfigautoDelete:# Auto delete of Cluster-BoM (optional)clusterBomAge:120# Custer-BoM age in minutes# Cluster-BoM is deleted automatically if the# corresponding shoot cluster is removed and the # Clustere-BoM is older than the specified clusterBomAge. applicationConfigs:# List of applications to be deployed in target cluster- id:yamlexample2 # ID of the application within this Cluster-BoMconfigType:kapp # Deployment via kapp controller typeSpecificData:# Spec of a YAML/KAPP App, see# https://github.com/k14s/kapp-controller/blob/develop/docs/app-spec.mdfetch:- git:ref:origin/developurl:https://github.com/k14s/k8s-simple-app-examplesubPath:config-step-1-minimaltemplate:- ytt:{}deploy:- kapp:{}# more application deployments can go here"
},
{
	"uri": "https://gardener.github.io/potter-docs/controller-docs-0.113.0/docs/plain-yaml-example/",
	"title": "Cluster-BoM Example with plain yaml K8s resources",
	"tags": [],
	"description": "",
	"content": "In this example, kubernetes resources specified by plain yaml files are deployed. This is a special case of kapp deployments described here.\nThe provided kubernetes yaml resources are deployed according to the rules specified here, e.g. CRDs and namspaces are deployed before namespaced resources.\nThe first example uses a zip file containing one yml file with different kubernetes resources, which is accessible via the URL https://storage.googleapis.com/hub-tarballs/integration-tests/zip-http-plain-yml/config.zip.\nOf course the zip might contain more than one yml file. For pitfalls with respect to the zip creation see the kapp example.\napiVersion:\u0026#34;hub.k8s.sap.com/v1\u0026#34;kind:ClusterBommetadata:name:demo # Cluster-BoM name.namespace: garden-apphubdemo # Cluster-BoM namespace. Pattern:garden-\u0026lt;projectname in Gardener\u0026gt;spec:secretRef:my-cluster.kubeconfig # Reference to kubeconfig of target cluster # Pattern: \u0026lt;name of Kubernetes cluster in gardener\u0026gt;.kubeconfigautoDelete:# Auto delete of Cluster-BoM (optional)clusterBomAge:120# Custer-BoM age in minutes# Cluster-BoM is deleted automatically if the# corresponding shoot cluster is removed and the # Clustere-BoM is older than the specified clusterBomAge. applicationConfigs:# List of applications to be deployed in target cluster- id:yamlexample1 # ID of the application within this Cluster-BoMconfigType:kapp # Deployment via kapp controller typeSpecificData:# Spec of a YAML/KAPP App, see# https://github.com/k14s/kapp-controller/blob/develop/docs/app-spec.mdfetch:- http:url:https://storage.googleapis.com/hub-tarballs/integration-tests/zip-http-plain-yml/config.ziptemplate:# No templating- ytt:{}deploy:- kapp:{}# more application deployments can go hereThe second example fetches the yml files from a git repository. Of course the specified folder in the Git repository might contain more than one yml file.\napiVersion:\u0026#34;hub.k8s.sap.com/v1\u0026#34;kind:ClusterBommetadata:name:demo # Cluster-BoM name.namespace: garden-apphubdemo # Cluster-BoM namespace. Pattern:garden-\u0026lt;projectname in Gardener\u0026gt;spec:secretRef:my-cluster.kubeconfig # Reference to kubeconfig of target cluster # Pattern: \u0026lt;name of Kubernetes cluster in gardener\u0026gt;.kubeconfigautoDelete:# Auto delete of Cluster-BoM (optional)clusterBomAge:120# Custer-BoM age in minutes# Cluster-BoM is deleted automatically if the# corresponding shoot cluster is removed and the # Clustere-BoM is older than the specified clusterBomAge. applicationConfigs:# List of applications to be deployed in target cluster- id:yamlexample2 # ID of the application within this Cluster-BoMconfigType:kapp # Deployment via kapp controller typeSpecificData:# Spec of a YAML/KAPP App, see# https://github.com/k14s/kapp-controller/blob/develop/docs/app-spec.mdfetch:- git:ref:origin/developurl:https://github.com/k14s/k8s-simple-app-examplesubPath:config-step-1-minimaltemplate:- ytt:{}deploy:- kapp:{}# more application deployments can go here"
},
{
	"uri": "https://gardener.github.io/potter-docs/controller-docs/docs/plain-yaml-example/",
	"title": "Cluster-BoM Example with plain yaml K8s resources",
	"tags": [],
	"description": "",
	"content": "In this example, kubernetes resources specified by plain yaml files are deployed. This is a special case of kapp deployments described here.\nThe provided kubernetes yaml resources are deployed according to the rules specified here, e.g. CRDs and namspaces are deployed before namespaced resources.\nThe first example uses a zip file containing one yml file with different kubernetes resources, which is accessible via the URL https://storage.googleapis.com/hub-tarballs/integration-tests/zip-http-plain-yml/config.zip.\nOf course the zip might contain more than one yml file. For pitfalls with respect to the zip creation see the kapp example.\napiVersion:\u0026#34;hub.k8s.sap.com/v1\u0026#34;kind:ClusterBommetadata:name:demo # Cluster-BoM name.namespace: garden-apphubdemo # Cluster-BoM namespace. Pattern:garden-\u0026lt;projectname in Gardener\u0026gt;spec:secretRef:my-cluster.kubeconfig # Reference to kubeconfig of target cluster # Pattern: \u0026lt;name of Kubernetes cluster in gardener\u0026gt;.kubeconfigautoDelete:# Auto delete of Cluster-BoM (optional)clusterBomAge:120# Custer-BoM age in minutes# Cluster-BoM is deleted automatically if the# corresponding shoot cluster is removed and the # Clustere-BoM is older than the specified clusterBomAge. applicationConfigs:# List of applications to be deployed in target cluster- id:yamlexample1 # ID of the application within this Cluster-BoMconfigType:kapp # Deployment via kapp controller typeSpecificData:# Spec of a YAML/KAPP App, see# https://github.com/k14s/kapp-controller/blob/develop/docs/app-spec.mdfetch:- http:url:https://storage.googleapis.com/hub-tarballs/integration-tests/zip-http-plain-yml/config.ziptemplate:# No templating- ytt:{}deploy:- kapp:{}# more application deployments can go hereThe second example fetches the yml files from a git repository. Of course the specified folder in the Git repository might contain more than one yml file.\napiVersion:\u0026#34;hub.k8s.sap.com/v1\u0026#34;kind:ClusterBommetadata:name:demo # Cluster-BoM name.namespace: garden-apphubdemo # Cluster-BoM namespace. Pattern:garden-\u0026lt;projectname in Gardener\u0026gt;spec:secretRef:my-cluster.kubeconfig # Reference to kubeconfig of target cluster # Pattern: \u0026lt;name of Kubernetes cluster in gardener\u0026gt;.kubeconfigautoDelete:# Auto delete of Cluster-BoM (optional)clusterBomAge:120# Custer-BoM age in minutes# Cluster-BoM is deleted automatically if the# corresponding shoot cluster is removed and the # Clustere-BoM is older than the specified clusterBomAge. applicationConfigs:# List of applications to be deployed in target cluster- id:yamlexample2 # ID of the application within this Cluster-BoMconfigType:kapp # Deployment via kapp controller typeSpecificData:# Spec of a YAML/KAPP App, see# https://github.com/k14s/kapp-controller/blob/develop/docs/app-spec.mdfetch:- git:ref:origin/developurl:https://github.com/k14s/k8s-simple-app-examplesubPath:config-step-1-minimaltemplate:- ytt:{}deploy:- kapp:{}# more application deployments can go here"
},
{
	"uri": "https://gardener.github.io/potter-docs/controller-docs-0.112.0/docs/status/",
	"title": "Cluster-BoM Status Section explained",
	"tags": [],
	"description": "",
	"content": " Status Section Overview Cluster-BoM Status Section Example Conditions Deployment State of each application Overall Deployment State  Status Section Overview To check the deployment status, use kubectl get clusterbom [NAME-OF-YOUR-CLUSTERBOM] -o yaml. Kubernetes returns the current metadata of the BoM resource. This metadata includes information about the last applied configuration, your applicationConfigs, and a status section at the end.\nThe status section consists of:\n  the conditions describing if all configured apps are up and running in their last revision and if all removed applications have been deleted from the target cluster. This is the most important part in the status for the end user. Other parts of the status are more for detailed problem analysis.\n  the applicationStates with a list of detailedState items containing a detailed description of the deployment and running state for every application specified by the Cluster-BoM. It reflects what currently is deployed on the target cluster in comparison to what is specified in the Cluster-BoM. This might also include detailed states of applications which are already removed from the Cluster-BoM but not already removed from the target cluster.\n  the overallState that is an aggregated state of all application states.\n  Execute kubectl get clusterbom [NAME-OF-YOUR-CLUSTERBOM] -o yaml several times to see the changes of your deployment status.\n The explanatory comments are not part of the output.\n Cluster-BoM Status Section Example status:conditions:- lastTransitionTime:\u0026#34;2020-04-07T08:38:57Z\u0026#34;lastUpdateTime:\u0026#34;2020-04-08T06:03:50Z\u0026#34;message: \u0026#39;Pending applications:mongodb, servicecatalog. \u0026#39;reason:PendingAppsstatus:Unknowntype:ReadyapplicationStates:- detailedState:deletionTimestamp:\u0026#34;2020-03-13T09:11:28Z\u0026#34;# indicator that this app is marked for deletiongeneration:1lastOperation:description:install successfulobservedGeneration:1successGeneration:1numberOfTries:1operation:installstate:ok # state of last install/removetime:\u0026#34;2020-03-13T09:11:22Z\u0026#34;reachability:reachable:truetime:\u0026#34;2020-03-13T09:11:22Z\u0026#34;readiness:state:oktime:\u0026#34;2020-03-13T09:11:23Z\u0026#34;id:karydiastate:ok # application state- detailedState:generation:1lastOperation:description:install successfulobservedGeneration:1successGeneration:1numberOfTries:1operation:installstate:ok # state of last install/removetime:\u0026#34;2020-03-13T09:11:23Z\u0026#34;reachability:reachable:truetime:\u0026#34;2020-03-13T09:11:23Z\u0026#34;readiness:state:pendingtime:\u0026#34;2020-03-13T09:11:23Z\u0026#34;id:mongodbstate:ok # application state overallState:pending # overall stateoverallTime:\u0026#34;2020-04-08T06:03:50Z\u0026#34;# last update time of overall stateConditions The condition of type Ready describes if all configured apps are up and running in their last revision.\n   Section Field Description     type Ready   status True: All configured apps are up and running in their last revision, the specified jobs in the readyRequirements sections are finished successfully and all removed applications have been deleted from the target cluster False: Some application has failed without any chance to recover. Unknown: Otherwise.    Every Cluster-BoM contains a generation field in its metadata section which is automatically increased by the Kubernetes environment when a modification is done. In this sense it describes the revision of the complete spec section of the Cluster-BoM.\nIn the status section there is a field observedGeneration which describes the revision or generation to which the condition refers. Only if the observedGeneration and the generation contain identical numbers, the condition describes the status of the current Cluster-BoM and not of a former state/revision.\nThe main advantage of the Ready condition compared to the overall deployment state is that it is computed with respect to a particular revision of a Cluster-BoM. The overall deployment state is computed based on an internal snapshot of what is currently deployed and what was internally seen as the last revision of particular applications.\nThe condition of type ClusterReachable describes if the target cluster is reachable.\n   Section Field Description     type ClusterReachable   status True: Target Shoot Cluster is reachable. False: Target Shoot Cluster is not reachable. Unknown: No information available.    Deployment State of Each Application The detailedState of an application consists of:\n  generation: Revision number for the application and its settings (for example, version, values), that should be deployed on the target shoot cluster. This number is automatically increased every time a user make changes to the application configuration. There might be a small delay until changes in the Cluster-BoM are reflected in changes of the generation of the affected application configurations.\n  deletionTimestamp: If this entry is set, the corresponding application is marked for deletion and will be removed after the application was uninstalled from the target cluster.\n  lastOperation:\nDescribes the status of the last applied installation or removal operation with respect to the application config in the Cluster-BoM.\n   Section Field Description     operation Operation that was triggered in this revision, for example, install or remove.   observedGeneration Revision number (generation) for the application and its settings (for example, version, values), that was installed or removed on the target shoot cluster. A lower number compared to generation signals that no the latest setting where applied until now.   numberOfTries The Application Hub doesn’t stop to try the deployment or removal if there are issues, but waits longer after each attempt before trying it again.   state ok: the last try of the operation succeeded.\nfailed: The last try of the operation didn\u0026rsquo;t succeed.   successGeneration The last revision number (generation) for which the operation succeeded. Due to an internal reconcile loop which re-executes the last operation from time to time, the state only informs about the success or failure of this. With the successGeneration you see which was the last successfully applied revision.   description More details about the operation result.   time Timestamp describing when the revision was applied.   errorHistory The first and up to the 4 last errors with respect to the last applied revision.      reachability:\nDescribes the availability of the target cluster. Operations aren\u0026rsquo;t executed if the target cluster isn’t reachable, for example, if it’s hibernated. In such a case also section like lastOperation are not updated.\n   Section Field Description     reachable true: The cluster could be reached.\nfalse: The cluster couldn\u0026rsquo;t be reached.   time Time of the last check. Not reachable clusters are rechecked every couple of minutes.      readiness:\nThe lastOperation section describes the state with respect to the deployment and removal of k8s resources of the application. The readiness instead describes if the most important components of the installed applications are up and running. Currently all Deployments, DaemonSets and StatefulSets are checked for this. Furthermore the specified jobs of the readyRequirements section must be finished successfully.\n   Section Field Description     state ok: For the last deployed revision all relevant k8s components are up and running and all specified jobs of the readyRequirements section must be finished successfully. pending: For the last deployed revision not all relevant k8s components are already up and running. failed: The last deployment failed.\nfinallyFailed: One of the jobs specified in the readyRequirements section failed. notRelevant: For removal operations. unknown: If something failed when finding out the readiness state, e.g. access to the target cluster timed out.   time Time of the last check. Not reachable clusters are rechecked every couple of minutes.      detailedState.state: Overall state for one application computed as follows:\n   Current Operation Description     remove ok: The application was successfully uninstalled from the target cluster, i.e. lastOperation.operation is remove and lastOperation.state is ok.\npending: The uninstall operation was not executed until now, i.e. lastOperation.operation is not remove. failed: The uninstall operation failed but will be retried, i.e. lastOperation.operation is remove and lastOperation.state is not ok.   install ok: The last application which was tried to install, was successfully installed on the target cluster and is ready. The last tried application might not be the latest specified in the Cluster-BoM. pending: The last tried application was successfully installed but is not already up and running or there is newer revision of the application to be deployed. failed: The installation of the last revision of the application failed or some components of the applications failed to succeed.\nunknown: Something failed when finding out the state, e.g. access to the target cluster timed out.      typeSpecificStatus: Here you find additional status information depending on the config type (e.g. helm or kapp). Currently this is only used for kapp. More detailed information about the information provided here could be found here.\n  Overall Deployment State Beside the detailed state, status.overallState provides an aggregated state of the application states (of detailedState.state) as the following table exemplifies:\n   Single States:         application state 1 ok ok ok ok   application state 2 ok ok ok failed   application state 3 ok ok unknown unknown   application state 4 ok pending pending pending   Calculated Overall State:       status.overallState ok pending unknown failed    Besides this you find summarized information about the installation progress:\noverallNumOfDeployments:4# number of configured applicationsoverallNumOfReadyDeployments:3# number of successfully installed applicationsoverallProgress:75# percentage of successfully installed applications"
},
{
	"uri": "https://gardener.github.io/potter-docs/controller-docs-0.113.0/docs/status/",
	"title": "Cluster-BoM Status Section explained",
	"tags": [],
	"description": "",
	"content": " Status Section Overview Cluster-BoM Status Section Example Conditions Deployment State of each application Overall Deployment State  Status Section Overview To check the deployment status, use kubectl get clusterbom [NAME-OF-YOUR-CLUSTERBOM] -o yaml. Kubernetes returns the current metadata of the BoM resource. This metadata includes information about the last applied configuration, your applicationConfigs, and a status section at the end.\nThe status section consists of:\n  the conditions describing if all configured apps are up and running in their last revision and if all removed applications have been deleted from the target cluster. This is the most important part in the status for the end user. Other parts of the status are more for detailed problem analysis.\n  the applicationStates with a list of detailedState items containing a detailed description of the deployment and running state for every application specified by the Cluster-BoM. It reflects what currently is deployed on the target cluster in comparison to what is specified in the Cluster-BoM. This might also include detailed states of applications which are already removed from the Cluster-BoM but not already removed from the target cluster.\n  the overallState that is an aggregated state of all application states.\n  Execute kubectl get clusterbom [NAME-OF-YOUR-CLUSTERBOM] -o yaml several times to see the changes of your deployment status.\n The explanatory comments are not part of the output.\n Cluster-BoM Status Section Example status:conditions:- lastTransitionTime:\u0026#34;2020-04-07T08:38:57Z\u0026#34;lastUpdateTime:\u0026#34;2020-04-08T06:03:50Z\u0026#34;message: \u0026#39;Pending applications:mongodb, servicecatalog. \u0026#39;reason:PendingAppsstatus:Unknowntype:ReadyapplicationStates:- detailedState:deletionTimestamp:\u0026#34;2020-03-13T09:11:28Z\u0026#34;# indicator that this app is marked for deletiongeneration:1lastOperation:description:install successfulobservedGeneration:1successGeneration:1numberOfTries:1operation:installstate:ok # state of last install/removetime:\u0026#34;2020-03-13T09:11:22Z\u0026#34;reachability:reachable:truetime:\u0026#34;2020-03-13T09:11:22Z\u0026#34;readiness:state:oktime:\u0026#34;2020-03-13T09:11:23Z\u0026#34;id:karydiastate:ok # application state- detailedState:generation:1lastOperation:description:install successfulobservedGeneration:1successGeneration:1numberOfTries:1operation:installstate:ok # state of last install/removetime:\u0026#34;2020-03-13T09:11:23Z\u0026#34;reachability:reachable:truetime:\u0026#34;2020-03-13T09:11:23Z\u0026#34;readiness:state:pendingtime:\u0026#34;2020-03-13T09:11:23Z\u0026#34;id:mongodbstate:ok # application state overallState:pending # overall stateoverallTime:\u0026#34;2020-04-08T06:03:50Z\u0026#34;# last update time of overall stateConditions The condition of type Ready describes if all configured apps are up and running in their last revision.\n   Section Field Description     type Ready   status True: All configured apps are up and running in their last revision, the specified jobs in the readyRequirements sections are finished successfully and all removed applications have been deleted from the target cluster False: Some application has failed without any chance to recover. Unknown: Otherwise.    Every Cluster-BoM contains a generation field in its metadata section which is automatically increased by the Kubernetes environment when a modification is done. In this sense it describes the revision of the complete spec section of the Cluster-BoM.\nIn the status section there is a field observedGeneration which describes the revision or generation to which the condition refers. Only if the observedGeneration and the generation contain identical numbers, the condition describes the status of the current Cluster-BoM and not of a former state/revision.\nThe main advantage of the Ready condition compared to the overall deployment state is that it is computed with respect to a particular revision of a Cluster-BoM. The overall deployment state is computed based on an internal snapshot of what is currently deployed and what was internally seen as the last revision of particular applications.\nThe condition of type ClusterReachable describes if the target cluster is reachable.\n   Section Field Description     type ClusterReachable   status True: Target Shoot Cluster is reachable. False: Target Shoot Cluster is not reachable. Unknown: No information available.    Deployment State of Each Application The detailedState of an application consists of:\n  generation: Revision number for the application and its settings (for example, version, values), that should be deployed on the target shoot cluster. This number is automatically increased every time a user make changes to the application configuration. There might be a small delay until changes in the Cluster-BoM are reflected in changes of the generation of the affected application configurations.\n  deletionTimestamp: If this entry is set, the corresponding application is marked for deletion and will be removed after the application was uninstalled from the target cluster.\n  lastOperation:\nDescribes the status of the last applied installation or removal operation with respect to the application config in the Cluster-BoM.\n   Section Field Description     operation Operation that was triggered in this revision, for example, install or remove.   observedGeneration Revision number (generation) for the application and its settings (for example, version, values), that was installed or removed on the target shoot cluster. A lower number compared to generation signals that no the latest setting where applied until now.   numberOfTries The Application Hub doesn’t stop to try the deployment or removal if there are issues, but waits longer after each attempt before trying it again.   state ok: the last try of the operation succeeded.\nfailed: The last try of the operation didn\u0026rsquo;t succeed.   successGeneration The last revision number (generation) for which the operation succeeded. Due to an internal reconcile loop which re-executes the last operation from time to time, the state only informs about the success or failure of this. With the successGeneration you see which was the last successfully applied revision.   description More details about the operation result.   time Timestamp describing when the revision was applied.   errorHistory The first and up to the 4 last errors with respect to the last applied revision.      reachability:\nDescribes the availability of the target cluster. Operations aren\u0026rsquo;t executed if the target cluster isn’t reachable, for example, if it’s hibernated. In such a case also section like lastOperation are not updated.\n   Section Field Description     reachable true: The cluster could be reached.\nfalse: The cluster couldn\u0026rsquo;t be reached.   time Time of the last check. Not reachable clusters are rechecked every couple of minutes.      readiness:\nThe lastOperation section describes the state with respect to the deployment and removal of k8s resources of the application. The readiness instead describes if the most important components of the installed applications are up and running. Currently all Deployments, DaemonSets and StatefulSets are checked for this. Furthermore the specified jobs of the readyRequirements section must be finished successfully.\n   Section Field Description     state ok: For the last deployed revision all relevant k8s components are up and running and all specified jobs of the readyRequirements section must be finished successfully. pending: For the last deployed revision not all relevant k8s components are already up and running. failed: The last deployment failed.\nfinallyFailed: One of the jobs specified in the readyRequirements section failed. notRelevant: For removal operations. unknown: If something failed when finding out the readiness state, e.g. access to the target cluster timed out.   time Time of the last check. Not reachable clusters are rechecked every couple of minutes.      detailedState.state: Overall state for one application computed as follows:\n   Current Operation Description     remove ok: The application was successfully uninstalled from the target cluster, i.e. lastOperation.operation is remove and lastOperation.state is ok.\npending: The uninstall operation was not executed until now, i.e. lastOperation.operation is not remove. failed: The uninstall operation failed but will be retried, i.e. lastOperation.operation is remove and lastOperation.state is not ok.   install ok: The last application which was tried to install, was successfully installed on the target cluster and is ready. The last tried application might not be the latest specified in the Cluster-BoM. pending: The last tried application was successfully installed but is not already up and running or there is newer revision of the application to be deployed. failed: The installation of the last revision of the application failed or some components of the applications failed to succeed.\nunknown: Something failed when finding out the state, e.g. access to the target cluster timed out.      typeSpecificStatus: Here you find additional status information depending on the config type (e.g. helm or kapp). Currently this is only used for kapp. More detailed information about the information provided here could be found here.\n  Overall Deployment State Beside the detailed state, status.overallState provides an aggregated state of the application states (of detailedState.state) as the following table exemplifies:\n   Single States:         application state 1 ok ok ok ok   application state 2 ok ok ok failed   application state 3 ok ok unknown unknown   application state 4 ok pending pending pending   Calculated Overall State:       status.overallState ok pending unknown failed    Besides this you find summarized information about the installation progress:\noverallNumOfDeployments:4# number of configured applicationsoverallNumOfReadyDeployments:3# number of successfully installed applicationsoverallProgress:75# percentage of successfully installed applications"
},
{
	"uri": "https://gardener.github.io/potter-docs/controller-docs/docs/status/",
	"title": "Cluster-BoM Status Section explained",
	"tags": [],
	"description": "",
	"content": " Status Section Overview Cluster-BoM Status Section Example Conditions Deployment State of each application Overall Deployment State  Status Section Overview To check the deployment status, use kubectl get clusterbom [NAME-OF-YOUR-CLUSTERBOM] -o yaml. Kubernetes returns the current metadata of the BoM resource. This metadata includes information about the last applied configuration, your applicationConfigs, and a status section at the end.\nThe status section consists of:\n  the conditions describing if all configured apps are up and running in their last revision and if all removed applications have been deleted from the target cluster. This is the most important part in the status for the end user. Other parts of the status are more for detailed problem analysis.\n  the applicationStates with a list of detailedState items containing a detailed description of the deployment and running state for every application specified by the Cluster-BoM. It reflects what currently is deployed on the target cluster in comparison to what is specified in the Cluster-BoM. This might also include detailed states of applications which are already removed from the Cluster-BoM but not already removed from the target cluster.\n  the overallState that is an aggregated state of all application states.\n  Execute kubectl get clusterbom [NAME-OF-YOUR-CLUSTERBOM] -o yaml several times to see the changes of your deployment status.\n The explanatory comments are not part of the output.\n Cluster-BoM Status Section Example status:conditions:- lastTransitionTime:\u0026#34;2020-04-07T08:38:57Z\u0026#34;lastUpdateTime:\u0026#34;2020-04-08T06:03:50Z\u0026#34;message: \u0026#39;Pending applications:mongodb, servicecatalog. \u0026#39;reason:PendingAppsstatus:Unknowntype:ReadyapplicationStates:- detailedState:deletionTimestamp:\u0026#34;2020-03-13T09:11:28Z\u0026#34;# indicator that this app is marked for deletiongeneration:1lastOperation:description:install successfulobservedGeneration:1successGeneration:1numberOfTries:1operation:installstate:ok # state of last install/removetime:\u0026#34;2020-03-13T09:11:22Z\u0026#34;reachability:reachable:truetime:\u0026#34;2020-03-13T09:11:22Z\u0026#34;readiness:state:oktime:\u0026#34;2020-03-13T09:11:23Z\u0026#34;id:karydiastate:ok # application state- detailedState:generation:1lastOperation:description:install successfulobservedGeneration:1successGeneration:1numberOfTries:1operation:installstate:ok # state of last install/removetime:\u0026#34;2020-03-13T09:11:23Z\u0026#34;reachability:reachable:truetime:\u0026#34;2020-03-13T09:11:23Z\u0026#34;readiness:state:pendingtime:\u0026#34;2020-03-13T09:11:23Z\u0026#34;id:mongodbstate:ok # application state overallState:pending # overall stateoverallTime:\u0026#34;2020-04-08T06:03:50Z\u0026#34;# last update time of overall stateConditions The condition of type Ready describes if all configured apps are up and running in their last revision.\n   Section Field Description     type Ready   status True: All configured apps are up and running in their last revision, the specified jobs in the readyRequirements sections are finished successfully and all removed applications have been deleted from the target cluster False: Some application has failed without any chance to recover. Unknown: Otherwise.    Every Cluster-BoM contains a generation field in its metadata section which is automatically increased by the Kubernetes environment when a modification is done. In this sense it describes the revision of the complete spec section of the Cluster-BoM.\nIn the status section there is a field observedGeneration which describes the revision or generation to which the condition refers. Only if the observedGeneration and the generation contain identical numbers, the condition describes the status of the current Cluster-BoM and not of a former state/revision.\nThe main advantage of the Ready condition compared to the overall deployment state is that it is computed with respect to a particular revision of a Cluster-BoM. The overall deployment state is computed based on an internal snapshot of what is currently deployed and what was internally seen as the last revision of particular applications.\nThe condition of type ClusterReachable describes if the target cluster is reachable.\n   Section Field Description     type ClusterReachable   status True: Target Shoot Cluster is reachable. False: Target Shoot Cluster is not reachable. Unknown: No information available.    Deployment State of Each Application The detailedState of an application consists of:\n  generation: Revision number for the application and its settings (for example, version, values), that should be deployed on the target shoot cluster. This number is automatically increased every time a user make changes to the application configuration. There might be a small delay until changes in the Cluster-BoM are reflected in changes of the generation of the affected application configurations.\n  deletionTimestamp: If this entry is set, the corresponding application is marked for deletion and will be removed after the application was uninstalled from the target cluster.\n  lastOperation:\nDescribes the status of the last applied installation or removal operation with respect to the application config in the Cluster-BoM.\n   Section Field Description     operation Operation that was triggered in this revision, for example, install or remove.   observedGeneration Revision number (generation) for the application and its settings (for example, version, values), that was installed or removed on the target shoot cluster. A lower number compared to generation signals that no the latest setting where applied until now.   numberOfTries The Application Hub doesn’t stop to try the deployment or removal if there are issues, but waits longer after each attempt before trying it again.   state ok: the last try of the operation succeeded.\nfailed: The last try of the operation didn\u0026rsquo;t succeed.   successGeneration The last revision number (generation) for which the operation succeeded. Due to an internal reconcile loop which re-executes the last operation from time to time, the state only informs about the success or failure of this. With the successGeneration you see which was the last successfully applied revision.   description More details about the operation result.   time Timestamp describing when the revision was applied.   errorHistory The first and up to the 4 last errors with respect to the last applied revision.      reachability:\nDescribes the availability of the target cluster. Operations aren\u0026rsquo;t executed if the target cluster isn’t reachable, for example, if it’s hibernated. In such a case also section like lastOperation are not updated.\n   Section Field Description     reachable true: The cluster could be reached.\nfalse: The cluster couldn\u0026rsquo;t be reached.   time Time of the last check. Not reachable clusters are rechecked every couple of minutes.      readiness:\nThe lastOperation section describes the state with respect to the deployment and removal of k8s resources of the application. The readiness instead describes if the most important components of the installed applications are up and running. Currently all Deployments, DaemonSets and StatefulSets are checked for this. Furthermore the specified jobs of the readyRequirements section must be finished successfully.\n   Section Field Description     state ok: For the last deployed revision all relevant k8s components are up and running and all specified jobs of the readyRequirements section must be finished successfully. pending: For the last deployed revision not all relevant k8s components are already up and running. failed: The last deployment failed.\nfinallyFailed: One of the jobs specified in the readyRequirements section failed. notRelevant: For removal operations. unknown: If something failed when finding out the readiness state, e.g. access to the target cluster timed out.   time Time of the last check. Not reachable clusters are rechecked every couple of minutes.      detailedState.state: Overall state for one application computed as follows:\n   Current Operation Description     remove ok: The application was successfully uninstalled from the target cluster, i.e. lastOperation.operation is remove and lastOperation.state is ok.\npending: The uninstall operation was not executed until now, i.e. lastOperation.operation is not remove. failed: The uninstall operation failed but will be retried, i.e. lastOperation.operation is remove and lastOperation.state is not ok.   install ok: The last application which was tried to install, was successfully installed on the target cluster and is ready. The last tried application might not be the latest specified in the Cluster-BoM. pending: The last tried application was successfully installed but is not already up and running or there is newer revision of the application to be deployed. failed: The installation of the last revision of the application failed or some components of the applications failed to succeed.\nunknown: Something failed when finding out the state, e.g. access to the target cluster timed out.      typeSpecificStatus: Here you find additional status information depending on the config type (e.g. helm or kapp). Currently this is only used for kapp. More detailed information about the information provided here could be found here.\n  Overall Deployment State Beside the detailed state, status.overallState provides an aggregated state of the application states (of detailedState.state) as the following table exemplifies:\n   Single States:         application state 1 ok ok ok ok   application state 2 ok ok ok failed   application state 3 ok ok unknown unknown   application state 4 ok pending pending pending   Calculated Overall State:       status.overallState ok pending unknown failed    Besides this you find summarized information about the installation progress:\noverallNumOfDeployments:4# number of configured applicationsoverallNumOfReadyDeployments:3# number of successfully installed applicationsoverallProgress:75# percentage of successfully installed applications"
},
{
	"uri": "https://gardener.github.io/potter-docs/controller-docs-0.112.0/docs/template/",
	"title": "Cluster-BoM Template",
	"tags": [],
	"description": "",
	"content": "Use the below yaml template to start creating your own Cluster BoM. You can also refer to the Cluster BoM Helm Example to see all available parameters.\napiVersion:\u0026#34;hub.k8s.sap.com/v1\u0026#34;kind:ClusterBommetadata:name:\u0026lt;CLUSTER BOM NAME\u0026gt; namespace:\u0026lt;CLUSTER BOM NAMESPACE\u0026gt;spec:secretRef:\u0026lt;CLUSTER NAME\u0026gt;.kubeconfigapplicationConfigs:- id:\u0026lt;UNIQUE-ID\u0026gt;configType:\u0026lt;TYPE\u0026gt;values:typeSpecificData:"
},
{
	"uri": "https://gardener.github.io/potter-docs/controller-docs-0.113.0/docs/template/",
	"title": "Cluster-BoM Template",
	"tags": [],
	"description": "",
	"content": "Use the below yaml template to start creating your own Cluster BoM. You can also refer to the Cluster BoM Helm Example to see all available parameters.\napiVersion:\u0026#34;hub.k8s.sap.com/v1\u0026#34;kind:ClusterBommetadata:name:\u0026lt;CLUSTER BOM NAME\u0026gt; namespace:\u0026lt;CLUSTER BOM NAMESPACE\u0026gt;spec:secretRef:\u0026lt;CLUSTER NAME\u0026gt;.kubeconfigapplicationConfigs:- id:\u0026lt;UNIQUE-ID\u0026gt;configType:\u0026lt;TYPE\u0026gt;values:typeSpecificData:"
},
{
	"uri": "https://gardener.github.io/potter-docs/controller-docs/docs/template/",
	"title": "Cluster-BoM Template",
	"tags": [],
	"description": "",
	"content": "Use the below yaml template to start creating your own Cluster BoM. You can also refer to the Cluster BoM Helm Example to see all available parameters.\napiVersion:\u0026#34;hub.k8s.sap.com/v1\u0026#34;kind:ClusterBommetadata:name:\u0026lt;CLUSTER BOM NAME\u0026gt; namespace:\u0026lt;CLUSTER BOM NAMESPACE\u0026gt;spec:secretRef:\u0026lt;CLUSTER NAME\u0026gt;.kubeconfigapplicationConfigs:- id:\u0026lt;UNIQUE-ID\u0026gt;configType:\u0026lt;TYPE\u0026gt;values:typeSpecificData:"
},
{
	"uri": "https://gardener.github.io/potter-docs/controller-docs-0.112.0/docs/special-topics/",
	"title": "Special Topics",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gardener.github.io/potter-docs/controller-docs-0.113.0/docs/special-topics/",
	"title": "Special Topics",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gardener.github.io/potter-docs/controller-docs/docs/special-topics/",
	"title": "Special Topics",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gardener.github.io/potter-docs/contribute/",
	"title": "Contribute",
	"tags": [],
	"description": "Contributors guides for code and documentation",
	"content": "Code of conduct All members of the Gardener community must abide by the CNCF Code of Conduct. Only by respecting each other can we develop a productive, collaborative community. Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting gardener.opensource@sap.com and/or a Gardener project maintainer.\nContributing Gardener uses GitHub to manage reviews of pull requests.\n  If you are a new contributor see: Steps to Contribute\n  If you have a trivial fix or improvement, go ahead and create a pull request, addressing (with @...) a suitable maintainer of this repository (see CODEOWNERS of the repository you want to contribute to) in the description of the pull request.\n  If you plan to do something more involved, first discuss your ideas on our mailing list. This will avoid unnecessary work and surely give you and us a good deal of inspiration.\n  Relevant coding style guidelines are the Go Code Review Comments and the Formatting and style section of Peter Bourgon\u0026rsquo;s Go: Best Practices for Production Environments.\n  Steps to Contribute Should you wish to work on an issue, please claim it first by commenting on the GitHub issue that you want to work on it. This is to prevent duplicated efforts from contributors on the same issue.\nIf you have questions about one of the issues, with or without the tag, please comment on them and one of the maintainers will clarify it.\nIndividual Contributor License Agreement When you contribute (code, documentation, or anything else), be aware that we only accept contributions under the Gardener project\u0026rsquo;s license (see previous sections) and you need to agree to the Individual Contributor License Agreement. This applies to all contributors, including those contributing on behalf of a company. If you agree to its content, click on the link posted by the CLA assistant as a comment to the pull request. Click it to review the CLA, then accept it on the next screen if you agree to it. CLA assistant will save your decision for upcoming contributions and will notify you if there is any change to the CLA in the meantime.\nCorporate Contributor License Agreement If employees of a company contribute code, in addition to the individual agreement above, there needs to be one company agreement submitted. This is mainly for the protection of the contributing employees.\nAn authorized company representative needs to download, fill, and print the Corporate Contributor License Agreement form. Then either:\n Scan it and e-mail it to opensource@sap.com and gardener.opensource@sap.com Fax it to: +49 6227 78-45813 Send it by letter to: Industry Standards \u0026amp; Open Source Team, Dietmar-Hopp-Allee 16, 69190 Walldorf, Germany  The form contains a list of employees who are authorized to contribute on behalf of your company. When this list changes, please let us know.\nPull Request Checklist   Branch from the master branch and, if needed, rebase to the current master branch before submitting your pull request. If it doesn\u0026rsquo;t merge cleanly with master you may be asked to rebase your changes.\n  Commits should be as small as possible, while ensuring that each commit is correct independently (i.e., each commit should compile and pass tests).\n  If your patch is not getting reviewed or you need a specific person to review it, you can @-reply a reviewer asking for a review in the pull request or a comment, or you can ask for a review on our mailing list.\n  Add tests relevant to the fixed bug or new feature.\n  Issues and Planning We use GitHub issues to track bugs and enhancement requests and ZenHub for planning.\n Install the ZenHub Chrome plugin Login to ZenHub Open the Gardener ZenHub workspace  Security Release Process See Security Release Process\n"
},
{
	"uri": "https://gardener.github.io/potter-docs/controller-docs-0.112.0/docs/",
	"title": "Docs",
	"tags": [],
	"description": "",
	"content": "Welcome to the Potter Controller docs! Here, you can find everything you always wanted to know about the controller part of project Potter. As a first time visitor, you might want to first read through the Potter project overview. If you are looking for specific Potter Controller topics, please see the navigation on the left hand. The sources of the project could be found here.\n Working with Cluster-BoMs Prerequisites Procedure Events  To fully automate cluster deployments, files describing the bill of material for a cluster, shortly Cluster-BoMs, are used instead of the Hub UI. Such a Cluster-BoM is mainly a list of references to deployments plus their parameters. Currently Helm charts and kapp deployments are supported, but more formats will be added later.\nHelm charts are usually referenced with the help of Helm chart Repositories. As an alternative, it’s also possible to directly provide a link (URL) to a Helm chart (see the Cluster-BoM Helm example for more details).\nReferencing kapp deployments is described in the Cluster-BoM kapp example.\nHelm and kapp deployments could be mixed in one Cluster-BoM.\nWorking with Cluster-BoMs A Cluster-BoM is a Custom Resource (CR) and can therefore be used like any other standard Kubernetes resource. For example, after specifying your deployments in a yaml file, you can use the following kubectl commands as usual:\n   Command Usage     kubectl apply -f myclusterbom.yaml Apply your Cluster-BoM to a cluster of your project.   kubectl get clusterbom myclusterbom -o yaml Get information about the deployment status.   kubectl get clusterboms List all available Cluster-BoMs of your Gardener project.    Prerequisites   You have a running installation of Gardener and Potter and access to a Gardener project.\n  You have already a cluster in your Gardener project\n  You should use a personalized kubeconfig to access Gardener. In the Gardener dashboard, go to your account and from the Access widget download your personalized kubeconfig into ~/.kube/kubeconfig-garden[-myproject]. Follow the instructions to setup kubelogin, and create an alias like this:\nalias kgarden=\u0026#34;kubectl --kubeconfig ~/.kube/kubeconfig-garden-myproject.yaml\u0026#34;   For an automated script or pipeline, you can use a robot service account for the Gardener project where your cluster is located. To create new service accounts on the Gardener dashboard, choose Members \u0026gt; Service Accounts \u0026gt; +.\n Service accounts allow access to all project resources (including all shoot clusters). Deploying resources in a cluster using a Cluster-BoM is done in the context of a service account.\n   Procedure   To use the service account locally from your computer, choose your project on the Gardener dashboard, then MEMBERS \u0026gt; Service Accounts. Download the kubeconfig file of the service account you want to use.\n  Merge the downloaded kubeconfig of your service account to your existing ~/.kube/config file or set your KUBECONFIG variable accordingly.\n  Create a new file Cluster-BoM.yaml. To safe you some time, you can copy the example of the next section and adjust it to your needs.\n To find out which Helm charts are currently available, choose CLUSTERS \u0026gt; [YOUR-CLUSTER] \u0026gt; External Tools \u0026gt; K8s Applications and Services Hub on the Gardener dashboard. On the Hub UI, you can browse the catalog of available Helm charts. The list of available charts depends on the specific landscape configuration.\n   Edit your Cluster-BoM. If you use the example, you must at least change metadata.namespace and spec.secretRef:\n  Metadata for Cluster-BoM and kubeconfig of your target cluster:\n   Field Description Pattern     metadata.name Technical name of this Cluster-BoM The name of a Cluster-BoM must consist of lower case alphanumeric characters or . or -, must start and end with an alphanumeric character, and must not be longer than 63 characters (for example, testclusterbom.01).   metadata.namespace The namespace of this Cluster-BoM garden-[PROJECT-NAME-IN-GARDENER]   spec.secretRef Secret with the kubeconfig of your target cluster [TARGET-CLUSTER-NAME].kubeconfig      Mandatory Configuration Parameters for the application deployments (section applicationConfigs):\n   Field Description     id Unique ID of the application within this Cluster-BoM.\nPattern: ^[0-9a-z]{1,20}$   configType Type of deployment. Currently only helm and kapp is supported.       More detailed information about the Cluster-BoM Structure can be found in the examples for Helm and kapp applications.\n   After editing the Cluster-BoM file, use the Kubernetes context with your service account kubeconfig and execute the following command:\nkubectl apply -f [NAME-OF-YOUR-CLUSTERBOM].yaml\n If something is wrong within your Cluster-BoM, an error message is displayed.\n   To check if the deployments worked as expected:\n  In the context of your service account, get your deployed Cluster-BoM as yaml output:\nkubectl get clusterbom [NAME-OF-YOUR-CLUSTERBOM] -o yaml\nAt the end of the yaml output, there’s now a status section that includes the status of each individual deployment and the aggregated status of the complete Cluster-BoM.\nMore information: Cluster BoM Status\n  In the context of your shoot cluster (switch your current context), check if new pods are being created for the deployments specified in your Cluster-BoM:\nkubectl get pods\n    After this initial deployment, you can also do changes to the Cluster-BoM and apply the modified Cluster-BoM again in the context of your service account to your cluster, for example:\n To change the version of a referenced chart. To add or to delete individual deployments from the Cluster-BoM. To change Helm chart values.   Some values of the Cluster-BoM cannot be changed, for example, applicationConfigs.id.\n   To delete all deployments of a Cluster-BoM, just delete the whole Cluster-BoM using the following command in the context of your service account:\nkubectl delete clusterbom [NAME-OF-YOUR-CLUSTERBOM] --wait=false\n The Cluster-BoM is only marked for deletion using the Kubernetes finalizer concept. Only when all applications were successfully removed from the target cluster or when the target cluster is removed itself, the Cluster-BoM disappears as well. Use option --wait=true if you want to wait until this process is finished.\n   Events With the following command you see events for Cluster-BoMs:\n\u0026gt; kubectl describe clusterbom testbom ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessDeployment 12m (x3 over 33m) DeploymentController Reconcile of deployment done Normal SuccessDeployment 11m (x4 over 34m) DeploymentController Deployment ok ... "
},
{
	"uri": "https://gardener.github.io/potter-docs/controller-docs-0.113.0/docs/",
	"title": "Docs",
	"tags": [],
	"description": "",
	"content": "Welcome to the Potter Controller docs! Here, you can find everything you always wanted to know about the controller part of project Potter. As a first time visitor, you might want to first read through the Potter project overview. If you are looking for specific Potter Controller topics, please see the navigation on the left hand. The sources of the project could be found here.\n Working with Cluster-BoMs Prerequisites Procedure Events  To fully automate cluster deployments, files describing the bill of material for a cluster, shortly Cluster-BoMs, are used instead of the Hub UI. Such a Cluster-BoM is mainly a list of references to deployments plus their parameters. Currently Helm charts and kapp deployments are supported, but more formats will be added later.\nHelm charts are usually referenced with the help of Helm chart Repositories. As an alternative, it’s also possible to directly provide a link (URL) to a Helm chart (see the Cluster-BoM Helm example for more details).\nReferencing kapp deployments is described in the Cluster-BoM kapp example.\nHelm and kapp deployments could be mixed in one Cluster-BoM.\nWorking with Cluster-BoMs A Cluster-BoM is a Custom Resource (CR) and can therefore be used like any other standard Kubernetes resource. For example, after specifying your deployments in a yaml file, you can use the following kubectl commands as usual:\n   Command Usage     kubectl apply -f myclusterbom.yaml Apply your Cluster-BoM to a cluster of your project.   kubectl get clusterbom myclusterbom -o yaml Get information about the deployment status.   kubectl get clusterboms List all available Cluster-BoMs of your Gardener project.    Prerequisites   You have a running installation of Gardener and Potter and access to a Gardener project.\n  You have already a cluster in your Gardener project\n  You should use a personalized kubeconfig to access Gardener. In the Gardener dashboard, go to your account and from the Access widget download your personalized kubeconfig into ~/.kube/kubeconfig-garden[-myproject]. Follow the instructions to setup kubelogin, and create an alias like this:\nalias kgarden=\u0026#34;kubectl --kubeconfig ~/.kube/kubeconfig-garden-myproject.yaml\u0026#34;   For an automated script or pipeline, you can use a robot service account for the Gardener project where your cluster is located. To create new service accounts on the Gardener dashboard, choose Members \u0026gt; Service Accounts \u0026gt; +.\n Service accounts allow access to all project resources (including all shoot clusters). Deploying resources in a cluster using a Cluster-BoM is done in the context of a service account.\n   Procedure   To use the service account locally from your computer, choose your project on the Gardener dashboard, then MEMBERS \u0026gt; Service Accounts. Download the kubeconfig file of the service account you want to use.\n  Merge the downloaded kubeconfig of your service account to your existing ~/.kube/config file or set your KUBECONFIG variable accordingly.\n  Create a new file Cluster-BoM.yaml. To safe you some time, you can copy the example of the next section and adjust it to your needs.\n To find out which Helm charts are currently available, choose CLUSTERS \u0026gt; [YOUR-CLUSTER] \u0026gt; External Tools \u0026gt; K8s Applications and Services Hub on the Gardener dashboard. On the Hub UI, you can browse the catalog of available Helm charts. The list of available charts depends on the specific landscape configuration.\n   Edit your Cluster-BoM. If you use the example, you must at least change metadata.namespace and spec.secretRef:\n  Metadata for Cluster-BoM and kubeconfig of your target cluster:\n   Field Description Pattern     metadata.name Technical name of this Cluster-BoM The name of a Cluster-BoM must consist of lower case alphanumeric characters or . or -, must start and end with an alphanumeric character, and must not be longer than 63 characters (for example, testclusterbom.01).   metadata.namespace The namespace of this Cluster-BoM garden-[PROJECT-NAME-IN-GARDENER]   spec.secretRef Secret with the kubeconfig of your target cluster [TARGET-CLUSTER-NAME].kubeconfig      Mandatory Configuration Parameters for the application deployments (section applicationConfigs):\n   Field Description     id Unique ID of the application within this Cluster-BoM.\nPattern: ^[0-9a-z]{1,20}$   configType Type of deployment. Currently only helm and kapp is supported.       More detailed information about the Cluster-BoM Structure can be found in the examples for Helm and kapp applications.\n   After editing the Cluster-BoM file, use the Kubernetes context with your service account kubeconfig and execute the following command:\nkubectl apply -f [NAME-OF-YOUR-CLUSTERBOM].yaml\n If something is wrong within your Cluster-BoM, an error message is displayed.\n   To check if the deployments worked as expected:\n  In the context of your service account, get your deployed Cluster-BoM as yaml output:\nkubectl get clusterbom [NAME-OF-YOUR-CLUSTERBOM] -o yaml\nAt the end of the yaml output, there’s now a status section that includes the status of each individual deployment and the aggregated status of the complete Cluster-BoM.\nMore information: Cluster BoM Status\n  In the context of your shoot cluster (switch your current context), check if new pods are being created for the deployments specified in your Cluster-BoM:\nkubectl get pods\n    After this initial deployment, you can also do changes to the Cluster-BoM and apply the modified Cluster-BoM again in the context of your service account to your cluster, for example:\n To change the version of a referenced chart. To add or to delete individual deployments from the Cluster-BoM. To change Helm chart values.   Some values of the Cluster-BoM cannot be changed, for example, applicationConfigs.id.\n   To delete all deployments of a Cluster-BoM, just delete the whole Cluster-BoM using the following command in the context of your service account:\nkubectl delete clusterbom [NAME-OF-YOUR-CLUSTERBOM] --wait=false\n The Cluster-BoM is only marked for deletion using the Kubernetes finalizer concept. Only when all applications were successfully removed from the target cluster or when the target cluster is removed itself, the Cluster-BoM disappears as well. Use option --wait=true if you want to wait until this process is finished.\n   Events With the following command you see events for Cluster-BoMs:\n\u0026gt; kubectl describe clusterbom testbom ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessDeployment 12m (x3 over 33m) DeploymentController Reconcile of deployment done Normal SuccessDeployment 11m (x4 over 34m) DeploymentController Deployment ok ... "
},
{
	"uri": "https://gardener.github.io/potter-docs/controller-docs/docs/",
	"title": "Docs",
	"tags": [],
	"description": "",
	"content": "Welcome to the Potter Controller docs! Here, you can find everything you always wanted to know about the controller part of project Potter. As a first time visitor, you might want to first read through the Potter project overview. If you are looking for specific Potter Controller topics, please see the navigation on the left hand. The sources of the project could be found here.\n Working with Cluster-BoMs Prerequisites Procedure Events  To fully automate cluster deployments, files describing the bill of material for a cluster, shortly Cluster-BoMs, are used instead of the Hub UI. Such a Cluster-BoM is mainly a list of references to deployments plus their parameters. Currently Helm charts and kapp deployments are supported, but more formats will be added later.\nHelm charts are usually referenced with the help of Helm chart Repositories. As an alternative, it’s also possible to directly provide a link (URL) to a Helm chart (see the Cluster-BoM Helm example for more details).\nReferencing kapp deployments is described in the Cluster-BoM kapp example.\nHelm and kapp deployments could be mixed in one Cluster-BoM.\nWorking with Cluster-BoMs A Cluster-BoM is a Custom Resource (CR) and can therefore be used like any other standard Kubernetes resource. For example, after specifying your deployments in a yaml file, you can use the following kubectl commands as usual:\n   Command Usage     kubectl apply -f myclusterbom.yaml Apply your Cluster-BoM to a cluster of your project.   kubectl get clusterbom myclusterbom -o yaml Get information about the deployment status.   kubectl get clusterboms List all available Cluster-BoMs of your Gardener project.    Prerequisites   You have a running installation of Gardener and Potter and access to a Gardener project.\n  You have already a cluster in your Gardener project\n  You should use a personalized kubeconfig to access Gardener. In the Gardener dashboard, go to your account and from the Access widget download your personalized kubeconfig into ~/.kube/kubeconfig-garden[-myproject]. Follow the instructions to setup kubelogin, and create an alias like this:\nalias kgarden=\u0026#34;kubectl --kubeconfig ~/.kube/kubeconfig-garden-myproject.yaml\u0026#34;   For an automated script or pipeline, you can use a robot service account for the Gardener project where your cluster is located. To create new service accounts on the Gardener dashboard, choose Members \u0026gt; Service Accounts \u0026gt; +.\n Service accounts allow access to all project resources (including all shoot clusters). Deploying resources in a cluster using a Cluster-BoM is done in the context of a service account.\n   Procedure   To use the service account locally from your computer, choose your project on the Gardener dashboard, then MEMBERS \u0026gt; Service Accounts. Download the kubeconfig file of the service account you want to use.\n  Merge the downloaded kubeconfig of your service account to your existing ~/.kube/config file or set your KUBECONFIG variable accordingly.\n  Create a new file Cluster-BoM.yaml. To safe you some time, you can copy the example of the next section and adjust it to your needs.\n To find out which Helm charts are currently available, choose CLUSTERS \u0026gt; [YOUR-CLUSTER] \u0026gt; External Tools \u0026gt; K8s Applications and Services Hub on the Gardener dashboard. On the Hub UI, you can browse the catalog of available Helm charts. The list of available charts depends on the specific landscape configuration.\n   Edit your Cluster-BoM. If you use the example, you must at least change metadata.namespace and spec.secretRef:\n  Metadata for Cluster-BoM and kubeconfig of your target cluster:\n   Field Description Pattern     metadata.name Technical name of this Cluster-BoM The name of a Cluster-BoM must consist of lower case alphanumeric characters or . or -, must start and end with an alphanumeric character, and must not be longer than 63 characters (for example, testclusterbom.01).   metadata.namespace The namespace of this Cluster-BoM garden-[PROJECT-NAME-IN-GARDENER]   spec.secretRef Secret with the kubeconfig of your target cluster [TARGET-CLUSTER-NAME].kubeconfig      Mandatory Configuration Parameters for the application deployments (section applicationConfigs):\n   Field Description     id Unique ID of the application within this Cluster-BoM.\nPattern: ^[0-9a-z]{1,20}$   configType Type of deployment. Currently only helm and kapp is supported.       More detailed information about the Cluster-BoM Structure can be found in the examples for Helm and kapp applications.\n   After editing the Cluster-BoM file, use the Kubernetes context with your service account kubeconfig and execute the following command:\nkubectl apply -f [NAME-OF-YOUR-CLUSTERBOM].yaml\n If something is wrong within your Cluster-BoM, an error message is displayed.\n   To check if the deployments worked as expected:\n  In the context of your service account, get your deployed Cluster-BoM as yaml output:\nkubectl get clusterbom [NAME-OF-YOUR-CLUSTERBOM] -o yaml\nAt the end of the yaml output, there’s now a status section that includes the status of each individual deployment and the aggregated status of the complete Cluster-BoM.\nMore information: Cluster BoM Status\n  In the context of your shoot cluster (switch your current context), check if new pods are being created for the deployments specified in your Cluster-BoM:\nkubectl get pods\n    After this initial deployment, you can also do changes to the Cluster-BoM and apply the modified Cluster-BoM again in the context of your service account to your cluster, for example:\n To change the version of a referenced chart. To add or to delete individual deployments from the Cluster-BoM. To change Helm chart values.   Some values of the Cluster-BoM cannot be changed, for example, applicationConfigs.id.\n   To delete all deployments of a Cluster-BoM, just delete the whole Cluster-BoM using the following command in the context of your service account:\nkubectl delete clusterbom [NAME-OF-YOUR-CLUSTERBOM] --wait=false\n The Cluster-BoM is only marked for deletion using the Kubernetes finalizer concept. Only when all applications were successfully removed from the target cluster or when the target cluster is removed itself, the Cluster-BoM disappears as well. Use option --wait=true if you want to wait until this process is finished.\n   Events With the following command you see events for Cluster-BoMs:\n\u0026gt; kubectl describe clusterbom testbom ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessDeployment 12m (x3 over 33m) DeploymentController Reconcile of deployment done Normal SuccessDeployment 11m (x4 over 34m) DeploymentController Deployment ok ... "
},
{
	"uri": "https://gardener.github.io/potter-docs/hub-docs-0.112.0/docs/",
	"title": "Docs",
	"tags": [],
	"description": "",
	"content": "Welcome to the Potter Hub! Here, you can find everything you always wanted to know about the Potter Hub. As a first time visitor, you might want to first go through this document to get an overview over the Potter Hub. If you are looking for something specific, please see the navigation on the left hand.\n For information about Cluster-BoMs, visit the Controller Docs.\n What Is the Potter Hub? The Potter Hub is a Kubernetes extension to deploy and manage deployments in Kubernetes clusters. It’s a central component in a Kubernetes landscape and doesn’t require any additional components running in the target shoot cluster. Target groups are primarily teams who want to deploy Kubernetes workload in a Gardener-managed cluster.\nBenefits of using the Potter Hub  Possibility to have one single YAML file describing the full set of K8s Deployments for a cluster instead of using individual Command Scripts for each K8s Deployment and also managing those deployments individually. Simply create a \u0026ldquo;Bill-of-Material\u0026rdquo; containing multiple k8s deployments using different deployment technologies like Helm, kapp or plain yaml (planned) Establish a declarative approach for K8s deployments (using Cluster-BoMs) as the standard for K8s Workloads to eliminate the need of creating specific deployment scripts to deploy K8s workload to Clusters, which quickly becomes unmanageable, especially in private cloud scenarios No need to deal with Helm or kapp (as well as other k8s deployment technologies) in detail. Instead, let the Potter Hub be responsible for handling these deployment technology details Possibility to retrieve and use one single status (the Cluster-BoM Status), representing the overall state of all deployments in a cluster, instead of trying to compute the overall cluster deployment state on your own Dependency-Management between Cluster Deployments (planned), where for example the sequence of K8s deployments can be specified, even between different K8s deployment technologies like Helm, kapp or plain YAML files Import/Export Management between deployments (planned), where specific data created during deployment of component A is needed during deployment of Component B Cluster-BoMs represent the single source of truth of what is actually running on a specific cluster, making it possible to provide deployment statistics across Gardener Projects (\u0026ldquo;What exactly runs on managed Clusters ?\u0026quot;)  Architecture Overview What can be deployed? Supported Kubernetes deployments are currently based on Helm charts and kapp applications, but other types will be supported in the future.\nTo trigger deployments, two approaches are available:\n Manual deployments using the UI Automated deployments using Cluster-BoMs (see the Controller Docs).  The Helm charts are located in dedicated Helm chart repositories. These repos must be connected to a Potter Hub installation according to the installation guide.\nGithub Repository The github repository of the Potter Hub could be found here\n"
},
{
	"uri": "https://gardener.github.io/potter-docs/hub-docs-0.113.0/docs/",
	"title": "Docs",
	"tags": [],
	"description": "",
	"content": "Welcome to the Potter Hub! Here, you can find everything you always wanted to know about the Potter Hub. As a first time visitor, you might want to first go through this document to get an overview over the Potter Hub. If you are looking for something specific, please see the navigation on the left hand.\n For information about Cluster-BoMs, visit the Controller Docs.\n What Is the Potter Hub? The Potter Hub is a Kubernetes extension to deploy and manage deployments in Kubernetes clusters. It’s a central component in a Kubernetes landscape and doesn’t require any additional components running in the target shoot cluster. Target groups are primarily teams who want to deploy Kubernetes workload in a Gardener-managed cluster.\nBenefits of using the Potter Hub  Possibility to have one single YAML file describing the full set of K8s Deployments for a cluster instead of using individual Command Scripts for each K8s Deployment and also managing those deployments individually. Simply create a \u0026ldquo;Bill-of-Material\u0026rdquo; containing multiple k8s deployments using different deployment technologies like Helm, kapp or plain yaml (planned) Establish a declarative approach for K8s deployments (using Cluster-BoMs) as the standard for K8s Workloads to eliminate the need of creating specific deployment scripts to deploy K8s workload to Clusters, which quickly becomes unmanageable, especially in private cloud scenarios No need to deal with Helm or kapp (as well as other k8s deployment technologies) in detail. Instead, let the Potter Hub be responsible for handling these deployment technology details Possibility to retrieve and use one single status (the Cluster-BoM Status), representing the overall state of all deployments in a cluster, instead of trying to compute the overall cluster deployment state on your own Dependency-Management between Cluster Deployments (planned), where for example the sequence of K8s deployments can be specified, even between different K8s deployment technologies like Helm, kapp or plain YAML files Import/Export Management between deployments (planned), where specific data created during deployment of component A is needed during deployment of Component B Cluster-BoMs represent the single source of truth of what is actually running on a specific cluster, making it possible to provide deployment statistics across Gardener Projects (\u0026ldquo;What exactly runs on managed Clusters ?\u0026quot;)  Architecture Overview What can be deployed? Supported Kubernetes deployments are currently based on Helm charts and kapp applications, but other types will be supported in the future.\nTo trigger deployments, two approaches are available:\n Manual deployments using the UI Automated deployments using Cluster-BoMs (see the Controller Docs).  The Helm charts are located in dedicated Helm chart repositories. These repos must be connected to a Potter Hub installation according to the installation guide.\nGithub Repository The github repository of the Potter Hub could be found here\n"
},
{
	"uri": "https://gardener.github.io/potter-docs/hub-docs/docs/",
	"title": "Docs",
	"tags": [],
	"description": "",
	"content": "Welcome to the Potter Hub! Here, you can find everything you always wanted to know about the Potter Hub. As a first time visitor, you might want to first go through this document to get an overview over the Potter Hub. If you are looking for something specific, please see the navigation on the left hand.\n For information about Cluster-BoMs, visit the Controller Docs.\n What Is the Potter Hub? The Potter Hub is a Kubernetes extension to deploy and manage deployments in Kubernetes clusters. It’s a central component in a Kubernetes landscape and doesn’t require any additional components running in the target shoot cluster. Target groups are primarily teams who want to deploy Kubernetes workload in a Gardener-managed cluster.\nBenefits of using the Potter Hub  Possibility to have one single YAML file describing the full set of K8s Deployments for a cluster instead of using individual Command Scripts for each K8s Deployment and also managing those deployments individually. Simply create a \u0026ldquo;Bill-of-Material\u0026rdquo; containing multiple k8s deployments using different deployment technologies like Helm, kapp or plain yaml (planned) Establish a declarative approach for K8s deployments (using Cluster-BoMs) as the standard for K8s Workloads to eliminate the need of creating specific deployment scripts to deploy K8s workload to Clusters, which quickly becomes unmanageable, especially in private cloud scenarios No need to deal with Helm or kapp (as well as other k8s deployment technologies) in detail. Instead, let the Potter Hub be responsible for handling these deployment technology details Possibility to retrieve and use one single status (the Cluster-BoM Status), representing the overall state of all deployments in a cluster, instead of trying to compute the overall cluster deployment state on your own Dependency-Management between Cluster Deployments (planned), where for example the sequence of K8s deployments can be specified, even between different K8s deployment technologies like Helm, kapp or plain YAML files Import/Export Management between deployments (planned), where specific data created during deployment of component A is needed during deployment of Component B Cluster-BoMs represent the single source of truth of what is actually running on a specific cluster, making it possible to provide deployment statistics across Gardener Projects (\u0026ldquo;What exactly runs on managed Clusters ?\u0026quot;)  Architecture Overview What can be deployed? Supported Kubernetes deployments are currently based on Helm charts and kapp applications, but other types will be supported in the future.\nTo trigger deployments, two approaches are available:\n Manual deployments using the UI Automated deployments using Cluster-BoMs (see the Controller Docs).  The Helm charts are located in dedicated Helm chart repositories. These repos must be connected to a Potter Hub installation according to the installation guide.\nGithub Repository The github repository of the Potter Hub could be found here\n"
},
{
	"uri": "https://gardener.github.io/potter-docs/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gardener.github.io/potter-docs/controller-docs-0.112.0/docs/special-topics/fetching-resources-from-private-github-repo/",
	"title": "Fetching Resources From a private GitHub Repository",
	"tags": [],
	"description": "",
	"content": "Section Cluster BoM kapp Example describes how to deploy an application using the kapp controller. Now we explain the additional steps that are necessary if the application files are fetched from a private GitHub repository:\n Create a Deploy Key for the GitHub repository Create a secret in your Gardener project which contains the private key. Create a Cluster BoM with a reference to the secret.  Creating a Deploy Key First, create a Deploy Key for your private GitHub repository. Basically, you you use the ssh_keygen command to create a pair of a private and a public key.\n▶ ssh-keygen -t rsa -b 4096 -C \u0026lt;email\u0026gt; Next, you upload the public key in the section Settings \u0026gt; Deploy keys of the GitHub repository. There is a step-by-step description in the GitHub documentation:\n Deploy Keys Generating a New SSH Key  Secret Create a secret in your Gardener project containing the private key generated in the previous step. You can use the following command to create this secret (adjust secret name, namespace, and path):\n▶ kubectl create secret generic -n \u0026lt;namespace\u0026gt; \u0026lt;secret name\u0026gt; --from-file=ssh-privatekey=\u0026lt;path to private key\u0026gt; The resulting secret contains the base64 encoded private key at data.ssh-privatekey:\napiVersion:v1kind:Secretmetadata:name:\u0026lt;secret name\u0026gt;namespace:\u0026lt;namespace\u0026gt;data:ssh-privatekey:\u0026lt;base64 encoded content of the private key file\u0026gt;type:OpaqueCluster BoM Create a Cluster BoM for your your application. In the Cluster-Bom, add a reference to the secret created in the previous step (field fetch.git.secretRef in type specific data of the application config). Use the SSH URL of the repository (field fetch.git.url).\napiVersion:\u0026#34;hub.k8s.sap.com/v1\u0026#34;kind:ClusterBommetadata:name:demonamespace:garden-apphubdemospec:secretRef:my-cluster.kubeconfigautoDelete:clusterBomAge:120applicationConfigs:- id:kappexampleconfigType:kapptypeSpecificData:fetch:- git:ref:origin/masterurl:git@github.com:demo/my-private-repo.git# SSH URL of the repositorysubPath:my/demo/applicationsecretRef:name:\u0026lt;secret name\u0026gt;# name of the secret with private deploy keytemplate:- ytt:{}deploy:- kapp:{}"
},
{
	"uri": "https://gardener.github.io/potter-docs/controller-docs-0.113.0/docs/special-topics/fetching-resources-from-private-github-repo/",
	"title": "Fetching Resources From a private GitHub Repository",
	"tags": [],
	"description": "",
	"content": "Section Cluster BoM kapp Example describes how to deploy an application using the kapp controller. Now we explain the additional steps that are necessary if the application files are fetched from a private GitHub repository:\n Create a Deploy Key for the GitHub repository Create a secret in your Gardener project which contains the private key. Create a Cluster BoM with a reference to the secret.  Creating a Deploy Key First, create a Deploy Key for your private GitHub repository. Basically, you you use the ssh_keygen command to create a pair of a private and a public key.\n▶ ssh-keygen -t rsa -b 4096 -C \u0026lt;email\u0026gt; Next, you upload the public key in the section Settings \u0026gt; Deploy keys of the GitHub repository. There is a step-by-step description in the GitHub documentation:\n Deploy Keys Generating a New SSH Key  Secret Create a secret in your Gardener project containing the private key generated in the previous step. You can use the following command to create this secret (adjust secret name, namespace, and path):\n▶ kubectl create secret generic -n \u0026lt;namespace\u0026gt; \u0026lt;secret name\u0026gt; --from-file=ssh-privatekey=\u0026lt;path to private key\u0026gt; The resulting secret contains the base64 encoded private key at data.ssh-privatekey:\napiVersion:v1kind:Secretmetadata:name:\u0026lt;secret name\u0026gt;namespace:\u0026lt;namespace\u0026gt;data:ssh-privatekey:\u0026lt;base64 encoded content of the private key file\u0026gt;type:OpaqueCluster BoM Create a Cluster BoM for your your application. In the Cluster-Bom, add a reference to the secret created in the previous step (field fetch.git.secretRef in type specific data of the application config). Use the SSH URL of the repository (field fetch.git.url).\napiVersion:\u0026#34;hub.k8s.sap.com/v1\u0026#34;kind:ClusterBommetadata:name:demonamespace:garden-apphubdemospec:secretRef:my-cluster.kubeconfigautoDelete:clusterBomAge:120applicationConfigs:- id:kappexampleconfigType:kapptypeSpecificData:fetch:- git:ref:origin/masterurl:git@github.com:demo/my-private-repo.git# SSH URL of the repositorysubPath:my/demo/applicationsecretRef:name:\u0026lt;secret name\u0026gt;# name of the secret with private deploy keytemplate:- ytt:{}deploy:- kapp:{}"
},
{
	"uri": "https://gardener.github.io/potter-docs/controller-docs/docs/special-topics/fetching-resources-from-private-github-repo/",
	"title": "Fetching Resources From a private GitHub Repository",
	"tags": [],
	"description": "",
	"content": "Section Cluster BoM kapp Example describes how to deploy an application using the kapp controller. Now we explain the additional steps that are necessary if the application files are fetched from a private GitHub repository:\n Create a Deploy Key for the GitHub repository Create a secret in your Gardener project which contains the private key. Create a Cluster BoM with a reference to the secret.  Creating a Deploy Key First, create a Deploy Key for your private GitHub repository. Basically, you you use the ssh_keygen command to create a pair of a private and a public key.\n▶ ssh-keygen -t rsa -b 4096 -C \u0026lt;email\u0026gt; Next, you upload the public key in the section Settings \u0026gt; Deploy keys of the GitHub repository. There is a step-by-step description in the GitHub documentation:\n Deploy Keys Generating a New SSH Key  Secret Create a secret in your Gardener project containing the private key generated in the previous step. You can use the following command to create this secret (adjust secret name, namespace, and path):\n▶ kubectl create secret generic -n \u0026lt;namespace\u0026gt; \u0026lt;secret name\u0026gt; --from-file=ssh-privatekey=\u0026lt;path to private key\u0026gt; The resulting secret contains the base64 encoded private key at data.ssh-privatekey:\napiVersion:v1kind:Secretmetadata:name:\u0026lt;secret name\u0026gt;namespace:\u0026lt;namespace\u0026gt;data:ssh-privatekey:\u0026lt;base64 encoded content of the private key file\u0026gt;type:OpaqueCluster BoM Create a Cluster BoM for your your application. In the Cluster-Bom, add a reference to the secret created in the previous step (field fetch.git.secretRef in type specific data of the application config). Use the SSH URL of the repository (field fetch.git.url).\napiVersion:\u0026#34;hub.k8s.sap.com/v1\u0026#34;kind:ClusterBommetadata:name:demonamespace:garden-apphubdemospec:secretRef:my-cluster.kubeconfigautoDelete:clusterBomAge:120applicationConfigs:- id:kappexampleconfigType:kapptypeSpecificData:fetch:- git:ref:origin/masterurl:git@github.com:demo/my-private-repo.git# SSH URL of the repositorysubPath:my/demo/applicationsecretRef:name:\u0026lt;secret name\u0026gt;# name of the secret with private deploy keytemplate:- ytt:{}deploy:- kapp:{}"
},
{
	"uri": "https://gardener.github.io/potter-docs/controller-docs-0.112.0/docs/special-topics/gitops/",
	"title": "GitOps",
	"tags": [],
	"description": "",
	"content": "In this chapter we show simple GitOps scenarios generating and updating Cluster-BoMs stored in a GitHub repository including templating.\nSimple Example Without Templating Assume we want to deploy an Application-Cluster-BoM like the following:\napiVersion:hub.k8s.sap.com/v1kind:ClusterBommetadata:name:simple-grafananamespace:garden-apphubdemospec:applicationConfigs:- configType:helmid:grafanatypeSpecificData:catalogAccess:chartName:grafanachartVersion:5.0.0repo:stableinstallName:grafananamespace:my-target-cluster-namespacesecretRef:my-cluster.kubeconfigAssume this Application-Cluster-BoM exists in a GitHub repository. We want that changes of this Application-Cluster-BoM in the git repository are applied automatically, i.e. without executing a command manually. We also want that this is done soon after the change in the git repository, let\u0026rsquo;s say in about a minute afterwards. To achieve this, we do not create the Application-Cluster-BoM manually, but install it in our Gardener project via a second Cluster-BoM, called GitOps-Cluster-BoM. It looks as follows.\napiVersion:hub.k8s.sap.com/v1kind:ClusterBommetadata:name:gitops-simplenamespace:garden-apphubdemo # Same namespace as for the Application-Cluster-BoMspec:applicationConfigs:- configType:kappid:gitopstypeSpecificData:syncPeriod:1m # Intervall for checking changes of the Application-Cluster-BoMcluster:namespace:garden-apphubdemo # Must be the same namespace as in metadata.namespacefetch:- git:ref:origin/masterurl:\u0026lt;GITREPO-URL\u0026gt; # Git repository containing the Application-Cluster-BoMsubPath:gitops/simple/application# Folder containing the Application-Cluster-BoMtemplate:- ytt:{}deploy:- kapp:{}secretRef:my-robot-secret # Secret to create the Application-Cluster-BoM # in Gardener namespace garden-apphubdemo.# See section \u0026#34;Secret to Create Application-Cluster-BoM\u0026#34; below# how to create this secret.When you have deployed the GitOps-Cluster-BoM, the AppHub watches the specified folder in the git repositoy. It will fetch the Application-Cluster-BoM from there and deploy it in the Gardener namespace of the GitOps-Cluster-BoM. As soon as the Application-Cluster-BoM has been deployed, the specified application (in this example grafana) will be deployed on the target cluster. The sync period (spec.applicationConfigs.typeSpecificData.syncPeriod) defines how often the Application-Cluster-BoM is checked for changes in the git repository. For example, if you change the Application-Cluster-BoM by increasing the grafana chartVersion, then the AppHub recognizes this change within the sync period and will deploy the new version automatically.\nYou could store additional Application-Cluster-BoMs in the same folder as the first, and they will also be deployed by the same GitOps-Cluster-BoM.\nPlease note that in order to make this work, you need to create a secret in your Garden Project first, which is needed to create the Application-Cluster-BoM in the Garden Project. See this section for more information.\nTo access private repositories, see Fetching-Resources-From-a-Private-GitHub-Repository\nExample With Templating and Multiple Clusters In this example, we want to deploy the same application with different settings on a dev and a live cluster. For example, we want to deploy grafana in version 5.0.0 on a live cluster and in version 5.0.2 on a dev cluster. We use the following template application_clusterbom.yml for the Application-Cluster-BoM. The templating tool is ytt.\n#@ load(\u0026#34;@ytt:data\u0026#34;, \u0026#34;data\u0026#34;)---apiVersion:hub.k8s.sap.com/v1kind:ClusterBommetadata:name:#@ data.values.clusterbom_namenamespace:garden-apphubdemospec:applicationConfigs:- configType:helmid:grafanatypeSpecificData:catalogAccess:chartName:grafanachartVersion:#@ data.values.chart_versionrepo:stableinstallName:grafananamespace:my-target-cluster-namespacesecretRef:#@ data.values.secret_refFor the dev cluster we fill the template with values from the following values values_dev.ymlfile:\n#@data/values---clusterbom_name:multi-grafana-devchart_version:5.0.2secret_ref:dev.kubeconfigFor the live cluster we fill the template with values from the following values values_live.ymlfile:\n#@data/values---clusterbom_name:multi-grafana-livechart_version:5.0.0secret_ref:live.kubeconfigThe template and the values files are stored in a git repository with the following directory structure:\ngitops └── multi ├── base │ └── application_clusterbom.yml ├── dev │ └── values_dev.yml └── live └── values_live.yml To deploy the Application-Cluster-Bom for the dev landscape, we use the following GitOps-Cluster-BoM, which combines the Cluster-Bom template with the dev values.\napiVersion:hub.k8s.sap.com/v1kind:ClusterBommetadata:name:gitops-multinamespace:garden-apphubdemospec:applicationConfigs:- configType:kappid:gitopstypeSpecificData:syncPeriod:1mcluster:namespace:garden-apphubdemofetch:- git:ref:origin/masterurl:\u0026lt;GITREPO-URL\u0026gt;subPath:gitops/multitemplate:- ytt:paths:- base # Folder gitops/multi/base in the git repo contains the file application_clusterbom.yml- dev # Folder gitops/multi/dev in the git repo contains the file values_dev.ymldeploy:- kapp:{}secretRef:my-robot-secretFor the live landscape, use a second GitOps-Cluster-Bom which combines the Cluster-Bom template with the live values:\ntemplate:- ytt:paths:- base # Contains the Cluster-BoM template- live # Contains the live valuesThere are many more possibilities to template with ytt. For more details and further references, see Cluster-BoM-kapp-Example.\nSecret to Create an Application-Cluster-BoM The Application-Cluster-BoMs are created on the Gardener cluster. Therefore we need a secret with these privileges. To create this secret proceed as follows.\n Create a service account for your Gardener project in the Members section of the Gardener Dashboard: Download the kubeconfig for this service account. Create a new secret using the service account kubeconfig file. Use the same service account kubeconfig as context, so that the secret will be created in the Gardener project.  Example:\nkubectl create secret generic \u0026lt;SECRET-NAME\u0026gt; -n garden-\u0026lt;GARDEN-PROJECT-NAME\u0026gt; --from-file=kubeconfig=\u0026lt;PATH-TO-SERVICE-ACCOUNT-KUBECONFIG\u0026gt; "
},
{
	"uri": "https://gardener.github.io/potter-docs/controller-docs-0.113.0/docs/special-topics/gitops/",
	"title": "GitOps",
	"tags": [],
	"description": "",
	"content": "In this chapter we show simple GitOps scenarios generating and updating Cluster-BoMs stored in a GitHub repository including templating.\nSimple Example Without Templating Assume we want to deploy an Application-Cluster-BoM like the following:\napiVersion:hub.k8s.sap.com/v1kind:ClusterBommetadata:name:simple-grafananamespace:garden-apphubdemospec:applicationConfigs:- configType:helmid:grafanatypeSpecificData:catalogAccess:chartName:grafanachartVersion:5.0.0repo:stableinstallName:grafananamespace:my-target-cluster-namespacesecretRef:my-cluster.kubeconfigAssume this Application-Cluster-BoM exists in a GitHub repository. We want that changes of this Application-Cluster-BoM in the git repository are applied automatically, i.e. without executing a command manually. We also want that this is done soon after the change in the git repository, let\u0026rsquo;s say in about a minute afterwards. To achieve this, we do not create the Application-Cluster-BoM manually, but install it in our Gardener project via a second Cluster-BoM, called GitOps-Cluster-BoM. It looks as follows.\napiVersion:hub.k8s.sap.com/v1kind:ClusterBommetadata:name:gitops-simplenamespace:garden-apphubdemo # Same namespace as for the Application-Cluster-BoMspec:applicationConfigs:- configType:kappid:gitopstypeSpecificData:syncPeriod:1m # Intervall for checking changes of the Application-Cluster-BoMcluster:namespace:garden-apphubdemo # Must be the same namespace as in metadata.namespacefetch:- git:ref:origin/masterurl:\u0026lt;GITREPO-URL\u0026gt; # Git repository containing the Application-Cluster-BoMsubPath:gitops/simple/application# Folder containing the Application-Cluster-BoMtemplate:- ytt:{}deploy:- kapp:{}secretRef:my-robot-secret # Secret to create the Application-Cluster-BoM # in Gardener namespace garden-apphubdemo.# See section \u0026#34;Secret to Create Application-Cluster-BoM\u0026#34; below# how to create this secret.When you have deployed the GitOps-Cluster-BoM, the AppHub watches the specified folder in the git repositoy. It will fetch the Application-Cluster-BoM from there and deploy it in the Gardener namespace of the GitOps-Cluster-BoM. As soon as the Application-Cluster-BoM has been deployed, the specified application (in this example grafana) will be deployed on the target cluster. The sync period (spec.applicationConfigs.typeSpecificData.syncPeriod) defines how often the Application-Cluster-BoM is checked for changes in the git repository. For example, if you change the Application-Cluster-BoM by increasing the grafana chartVersion, then the AppHub recognizes this change within the sync period and will deploy the new version automatically.\nYou could store additional Application-Cluster-BoMs in the same folder as the first, and they will also be deployed by the same GitOps-Cluster-BoM.\nPlease note that in order to make this work, you need to create a secret in your Garden Project first, which is needed to create the Application-Cluster-BoM in the Garden Project. See this section for more information.\nTo access private repositories, see Fetching-Resources-From-a-Private-GitHub-Repository\nExample With Templating and Multiple Clusters In this example, we want to deploy the same application with different settings on a dev and a live cluster. For example, we want to deploy grafana in version 5.0.0 on a live cluster and in version 5.0.2 on a dev cluster. We use the following template application_clusterbom.yml for the Application-Cluster-BoM. The templating tool is ytt.\n#@ load(\u0026#34;@ytt:data\u0026#34;, \u0026#34;data\u0026#34;)---apiVersion:hub.k8s.sap.com/v1kind:ClusterBommetadata:name:#@ data.values.clusterbom_namenamespace:garden-apphubdemospec:applicationConfigs:- configType:helmid:grafanatypeSpecificData:catalogAccess:chartName:grafanachartVersion:#@ data.values.chart_versionrepo:stableinstallName:grafananamespace:my-target-cluster-namespacesecretRef:#@ data.values.secret_refFor the dev cluster we fill the template with values from the following values values_dev.ymlfile:\n#@data/values---clusterbom_name:multi-grafana-devchart_version:5.0.2secret_ref:dev.kubeconfigFor the live cluster we fill the template with values from the following values values_live.ymlfile:\n#@data/values---clusterbom_name:multi-grafana-livechart_version:5.0.0secret_ref:live.kubeconfigThe template and the values files are stored in a git repository with the following directory structure:\ngitops └── multi ├── base │ └── application_clusterbom.yml ├── dev │ └── values_dev.yml └── live └── values_live.yml To deploy the Application-Cluster-Bom for the dev landscape, we use the following GitOps-Cluster-BoM, which combines the Cluster-Bom template with the dev values.\napiVersion:hub.k8s.sap.com/v1kind:ClusterBommetadata:name:gitops-multinamespace:garden-apphubdemospec:applicationConfigs:- configType:kappid:gitopstypeSpecificData:syncPeriod:1mcluster:namespace:garden-apphubdemofetch:- git:ref:origin/masterurl:\u0026lt;GITREPO-URL\u0026gt;subPath:gitops/multitemplate:- ytt:paths:- base # Folder gitops/multi/base in the git repo contains the file application_clusterbom.yml- dev # Folder gitops/multi/dev in the git repo contains the file values_dev.ymldeploy:- kapp:{}secretRef:my-robot-secretFor the live landscape, use a second GitOps-Cluster-Bom which combines the Cluster-Bom template with the live values:\ntemplate:- ytt:paths:- base # Contains the Cluster-BoM template- live # Contains the live valuesThere are many more possibilities to template with ytt. For more details and further references, see Cluster-BoM-kapp-Example.\nSecret to Create an Application-Cluster-BoM The Application-Cluster-BoMs are created on the Gardener cluster. Therefore we need a secret with these privileges. To create this secret proceed as follows.\n Create a service account for your Gardener project in the Members section of the Gardener Dashboard: Download the kubeconfig for this service account. Create a new secret using the service account kubeconfig file. Use the same service account kubeconfig as context, so that the secret will be created in the Gardener project.  Example:\nkubectl create secret generic \u0026lt;SECRET-NAME\u0026gt; -n garden-\u0026lt;GARDEN-PROJECT-NAME\u0026gt; --from-file=kubeconfig=\u0026lt;PATH-TO-SERVICE-ACCOUNT-KUBECONFIG\u0026gt; "
},
{
	"uri": "https://gardener.github.io/potter-docs/controller-docs/docs/special-topics/gitops/",
	"title": "GitOps",
	"tags": [],
	"description": "",
	"content": "In this chapter we show simple GitOps scenarios generating and updating Cluster-BoMs stored in a GitHub repository including templating.\nSimple Example Without Templating Assume we want to deploy an Application-Cluster-BoM like the following:\napiVersion:hub.k8s.sap.com/v1kind:ClusterBommetadata:name:simple-grafananamespace:garden-apphubdemospec:applicationConfigs:- configType:helmid:grafanatypeSpecificData:catalogAccess:chartName:grafanachartVersion:5.0.0repo:stableinstallName:grafananamespace:my-target-cluster-namespacesecretRef:my-cluster.kubeconfigAssume this Application-Cluster-BoM exists in a GitHub repository. We want that changes of this Application-Cluster-BoM in the git repository are applied automatically, i.e. without executing a command manually. We also want that this is done soon after the change in the git repository, let\u0026rsquo;s say in about a minute afterwards. To achieve this, we do not create the Application-Cluster-BoM manually, but install it in our Gardener project via a second Cluster-BoM, called GitOps-Cluster-BoM. It looks as follows.\napiVersion:hub.k8s.sap.com/v1kind:ClusterBommetadata:name:gitops-simplenamespace:garden-apphubdemo # Same namespace as for the Application-Cluster-BoMspec:applicationConfigs:- configType:kappid:gitopstypeSpecificData:syncPeriod:1m # Intervall for checking changes of the Application-Cluster-BoMcluster:namespace:garden-apphubdemo # Must be the same namespace as in metadata.namespacefetch:- git:ref:origin/masterurl:\u0026lt;GITREPO-URL\u0026gt; # Git repository containing the Application-Cluster-BoMsubPath:gitops/simple/application# Folder containing the Application-Cluster-BoMtemplate:- ytt:{}deploy:- kapp:{}secretRef:my-robot-secret # Secret to create the Application-Cluster-BoM # in Gardener namespace garden-apphubdemo.# See section \u0026#34;Secret to Create Application-Cluster-BoM\u0026#34; below# how to create this secret.When you have deployed the GitOps-Cluster-BoM, the AppHub watches the specified folder in the git repositoy. It will fetch the Application-Cluster-BoM from there and deploy it in the Gardener namespace of the GitOps-Cluster-BoM. As soon as the Application-Cluster-BoM has been deployed, the specified application (in this example grafana) will be deployed on the target cluster. The sync period (spec.applicationConfigs.typeSpecificData.syncPeriod) defines how often the Application-Cluster-BoM is checked for changes in the git repository. For example, if you change the Application-Cluster-BoM by increasing the grafana chartVersion, then the AppHub recognizes this change within the sync period and will deploy the new version automatically.\nYou could store additional Application-Cluster-BoMs in the same folder as the first, and they will also be deployed by the same GitOps-Cluster-BoM.\nPlease note that in order to make this work, you need to create a secret in your Garden Project first, which is needed to create the Application-Cluster-BoM in the Garden Project. See this section for more information.\nTo access private repositories, see Fetching-Resources-From-a-Private-GitHub-Repository\nExample With Templating and Multiple Clusters In this example, we want to deploy the same application with different settings on a dev and a live cluster. For example, we want to deploy grafana in version 5.0.0 on a live cluster and in version 5.0.2 on a dev cluster. We use the following template application_clusterbom.yml for the Application-Cluster-BoM. The templating tool is ytt.\n#@ load(\u0026#34;@ytt:data\u0026#34;, \u0026#34;data\u0026#34;)---apiVersion:hub.k8s.sap.com/v1kind:ClusterBommetadata:name:#@ data.values.clusterbom_namenamespace:garden-apphubdemospec:applicationConfigs:- configType:helmid:grafanatypeSpecificData:catalogAccess:chartName:grafanachartVersion:#@ data.values.chart_versionrepo:stableinstallName:grafananamespace:my-target-cluster-namespacesecretRef:#@ data.values.secret_refFor the dev cluster we fill the template with values from the following values values_dev.ymlfile:\n#@data/values---clusterbom_name:multi-grafana-devchart_version:5.0.2secret_ref:dev.kubeconfigFor the live cluster we fill the template with values from the following values values_live.ymlfile:\n#@data/values---clusterbom_name:multi-grafana-livechart_version:5.0.0secret_ref:live.kubeconfigThe template and the values files are stored in a git repository with the following directory structure:\ngitops └── multi ├── base │ └── application_clusterbom.yml ├── dev │ └── values_dev.yml └── live └── values_live.yml To deploy the Application-Cluster-Bom for the dev landscape, we use the following GitOps-Cluster-BoM, which combines the Cluster-Bom template with the dev values.\napiVersion:hub.k8s.sap.com/v1kind:ClusterBommetadata:name:gitops-multinamespace:garden-apphubdemospec:applicationConfigs:- configType:kappid:gitopstypeSpecificData:syncPeriod:1mcluster:namespace:garden-apphubdemofetch:- git:ref:origin/masterurl:\u0026lt;GITREPO-URL\u0026gt;subPath:gitops/multitemplate:- ytt:paths:- base # Folder gitops/multi/base in the git repo contains the file application_clusterbom.yml- dev # Folder gitops/multi/dev in the git repo contains the file values_dev.ymldeploy:- kapp:{}secretRef:my-robot-secretFor the live landscape, use a second GitOps-Cluster-Bom which combines the Cluster-Bom template with the live values:\ntemplate:- ytt:paths:- base # Contains the Cluster-BoM template- live # Contains the live valuesThere are many more possibilities to template with ytt. For more details and further references, see Cluster-BoM-kapp-Example.\nSecret to Create an Application-Cluster-BoM The Application-Cluster-BoMs are created on the Gardener cluster. Therefore we need a secret with these privileges. To create this secret proceed as follows.\n Create a service account for your Gardener project in the Members section of the Gardener Dashboard: Download the kubeconfig for this service account. Create a new secret using the service account kubeconfig file. Use the same service account kubeconfig as context, so that the secret will be created in the Gardener project.  Example:\nkubectl create secret generic \u0026lt;SECRET-NAME\u0026gt; -n garden-\u0026lt;GARDEN-PROJECT-NAME\u0026gt; --from-file=kubeconfig=\u0026lt;PATH-TO-SERVICE-ACCOUNT-KUBECONFIG\u0026gt; "
},
{
	"uri": "https://gardener.github.io/potter-docs/controller-docs-0.112.0/",
	"title": "Home",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gardener.github.io/potter-docs/controller-docs-0.113.0/",
	"title": "Home",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gardener.github.io/potter-docs/controller-docs/",
	"title": "Home",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gardener.github.io/potter-docs/hub-docs-0.112.0/",
	"title": "Home",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gardener.github.io/potter-docs/hub-docs-0.113.0/",
	"title": "Home",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gardener.github.io/potter-docs/hub-docs/",
	"title": "Home",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gardener.github.io/potter-docs/controller-docs-0.112.0/docs/special-topics/manual-reconcile/",
	"title": "Manual Reconcile",
	"tags": [],
	"description": "",
	"content": "You can trigger a reconcile for a Cluster-BoM by adding the following annotation. If this annotation is set, a reconcile loop is triggered for all apps of the Cluster-BoM except those for which automatic reconcile is disabled. The annotation is removed automatically after this operation.\nmetadata:annotations:hub.k8s.sap.com/reconcile:reconcile"
},
{
	"uri": "https://gardener.github.io/potter-docs/controller-docs-0.113.0/docs/special-topics/manual-reconcile/",
	"title": "Manual Reconcile",
	"tags": [],
	"description": "",
	"content": "You can trigger a reconcile for a Cluster-BoM by adding the following annotation. If this annotation is set, a reconcile loop is triggered for all apps of the Cluster-BoM except those for which automatic reconcile is disabled. The annotation is removed automatically after this operation.\nmetadata:annotations:hub.k8s.sap.com/reconcile:reconcile"
},
{
	"uri": "https://gardener.github.io/potter-docs/controller-docs/docs/special-topics/manual-reconcile/",
	"title": "Manual Reconcile",
	"tags": [],
	"description": "",
	"content": "You can trigger a reconcile for a Cluster-BoM by adding the following annotation. If this annotation is set, a reconcile loop is triggered for all apps of the Cluster-BoM except those for which automatic reconcile is disabled. The annotation is removed automatically after this operation.\nmetadata:annotations:hub.k8s.sap.com/reconcile:reconcile"
},
{
	"uri": "https://gardener.github.io/potter-docs/controller-docs-0.112.0/docs/special-topics/named-secrets/",
	"title": "Named Secrets",
	"tags": [],
	"description": "",
	"content": "Basics and Example Named secret values allow to specify secret values in a Cluster-BoM which are automatically moved to secrets before the Cluster-BoM is stored or updated. These secrets could be referenced in kapp deployments via their logical names. In helm deployments the secret values stored in the secrets are added as additional values files for templating.\nThe following example shows a Cluster-BoM with a section namedSecretValues with two named secret values potter-examples-access and otherSecrets. Make sure that the values of the top level map entries under data are strings.\napiVersion:hub.k8s.sap.com/v1kind:ClusterBommetadata:name:pot-namedsecretvalues-kappnamespace:\u0026lt;some-garden-namespace\u0026gt;spec:applicationConfigs:- configType:kappid:appId1namedSecretValues:potter-examples-access:# logical secret namedata:username:YOUR_USER # moved to secretpassword:YOUR_ACCESS_TOKEN # moved to secretotherSecrets:data:otherSecret1:|secret1: s11 secret2: s21otherSecret2:|secret3: user: users3 password: pw3typeSpecificData:fetch:- git:ref:origin/mastersubPath:namedsecretvalues/kapp/resourcesurl:https://github.com/your-path/potter-examplessecretRef:name:potter-examples-accesstemplate:- ytt:{}deploy:- kapp:{}secretRef:\u0026lt;clusterName\u0026gt;.kubeconfigWhen this Cluster-BoM is deployed, the data of potter-examples-access and otherSecrets are stored in two automatically generated secrets in the following format:\napiVersion:v1kind:Secretmetadata:labels:# internal secret name, not the logical secret name name:pot-namedsecretvalues-kapp-appId1-\u0026lt;someGuid1\u0026gt;namespace:\u0026lt;some-garden-namespace\u0026gt;type:Opaquedata:username:YOUR_USER password:YOUR_ACCESS_TOKENapiVersion:v1kind:Secretmetadata:labels:# internal secret name, not the logical secret name name:pot-namedsecretvalues-kapp-appId1-\u0026lt;someGuid2\u0026gt;namespace:\u0026lt;some-garden-namespace\u0026gt;type:Opaquedata:otherSecret1:secret1:s11secret2:s21otherSecret2:secret3:user:users3password:pw3The stored Cluster-BoM itself only contains references to these secrets instead of the data:\napiVersion:hub.k8s.sap.com/v1kind:ClusterBommetadata:name:pot-namedsecretvalues-kappnamespace:\u0026lt;some-garden-namespace\u0026gt;spec:applicationConfigs:- configType:kappid:appId1namedSecretValues:potter-examples-access:internalSecretName:pot-namedsecretvalues-kapp-appId1-\u0026lt;someGuid1\u0026gt;otherSecrets:internalSecretName:pot-namedsecretvalues-kapp-appId1-\u0026lt;someGuid2\u0026gt;typeSpecificData:fetch:- git:ref:origin/mastersubPath:namedsecretvalues/kapp/resourcesurl:https://github.com/your-path/potter-examplessecretRef:name:potter-examples-accesstemplate:- ytt:{}deploy:- kapp:{}secretRef:\u0026lt;clusterName\u0026gt;.kubeconfigUsage of Named Secrets in kapp deployments In the section typeSpecificData you see an example how named secret values could be used in a kapp deployment. Here the logical secret name potter-examples-access is specified as the name of a secret ref. During the deployment the secret connected with this logical name is used to access a private git repository. You could enter logical secret names everywhere in the type specific data section of a kapp deployment where a secret ref could be used.\nUsage of Named Secrets in helm deployments In helm deployments, the data of every named secret is provided as an additional values file during templating. These template files are applied after the values specified in the secret values section. There is no predefined order in which the data of the different named secrets are applied.\nA named secret could be also referenced in a tarball access as in the following example:\napiVersion:hub.k8s.sap.com/v1kind:ClusterBommetadata:name:pot-namedsecretvalues-authheadernamespace:\u0026lt;some-garden-namespace\u0026gt;spec:applicationConfigs:- configType:helmid:echoservernamedSecretValues:logical-name-1:data:authHeader:\u0026#34;Basic dX...3dvcmQ=\u0026#34;typeSpecificData:tarballAccess:url:\u0026#34;https://yourpath/echo-server-1.0.5.tgz\u0026#34;secretRef:name:logical-name-1installName:echoservernamespace:\u0026lt;some-target-namespace\u0026gt;secretRef:\u0026lt;your-clustername\u0026gt;.kubeconfigWith this a secret containing the authheader data is automatically created and used for accessing the tgz file. This secret it automatically excluded from being merged as an additional values file during helm template.\nUpdate Secret Values To keep the values of a named secret value with logical name X unchanged, there are several possibilities how to specify this in a Cluster-BoM. Either there is no namedSecretValues section at all or it does not contain X. You could also provide the named secret with identical data or no data.\nTo replace the secret values just provide new data.\nnamedSecretValues:X:data:...# new dataIt is not allowed to delete named secrets values by just providing an empty data section to prevent accidental deletions.\nDelete Secret Values To delete the secret values use the delete operation:\nnamedSecretValues:X:operation:delete"
},
{
	"uri": "https://gardener.github.io/potter-docs/controller-docs-0.113.0/docs/special-topics/named-secrets/",
	"title": "Named Secrets",
	"tags": [],
	"description": "",
	"content": "Basics and Example Named secret values allow to specify secret values in a Cluster-BoM which are automatically moved to secrets before the Cluster-BoM is stored or updated. These secrets could be referenced in kapp deployments via their logical names. In helm deployments the secret values stored in the secrets are added as additional values files for templating.\nThe following example shows a Cluster-BoM with a section namedSecretValues with two named secret values potter-examples-access and otherSecrets. Make sure that the values of the top level map entries under data are strings.\napiVersion:hub.k8s.sap.com/v1kind:ClusterBommetadata:name:pot-namedsecretvalues-kappnamespace:\u0026lt;some-garden-namespace\u0026gt;spec:applicationConfigs:- configType:kappid:appId1namedSecretValues:potter-examples-access:# logical secret namedata:username:YOUR_USER # moved to secretpassword:YOUR_ACCESS_TOKEN # moved to secretotherSecrets:data:otherSecret1:|secret1: s11 secret2: s21otherSecret2:|secret3: user: users3 password: pw3typeSpecificData:fetch:- git:ref:origin/mastersubPath:namedsecretvalues/kapp/resourcesurl:https://github.com/your-path/potter-examplessecretRef:name:potter-examples-accesstemplate:- ytt:{}deploy:- kapp:{}secretRef:\u0026lt;clusterName\u0026gt;.kubeconfigWhen this Cluster-BoM is deployed, the data of potter-examples-access and otherSecrets are stored in two automatically generated secrets in the following format:\napiVersion:v1kind:Secretmetadata:labels:# internal secret name, not the logical secret name name:pot-namedsecretvalues-kapp-appId1-\u0026lt;someGuid1\u0026gt;namespace:\u0026lt;some-garden-namespace\u0026gt;type:Opaquedata:username:YOUR_USER password:YOUR_ACCESS_TOKENapiVersion:v1kind:Secretmetadata:labels:# internal secret name, not the logical secret name name:pot-namedsecretvalues-kapp-appId1-\u0026lt;someGuid2\u0026gt;namespace:\u0026lt;some-garden-namespace\u0026gt;type:Opaquedata:otherSecret1:secret1:s11secret2:s21otherSecret2:secret3:user:users3password:pw3The stored Cluster-BoM itself only contains references to these secrets instead of the data:\napiVersion:hub.k8s.sap.com/v1kind:ClusterBommetadata:name:pot-namedsecretvalues-kappnamespace:\u0026lt;some-garden-namespace\u0026gt;spec:applicationConfigs:- configType:kappid:appId1namedSecretValues:potter-examples-access:internalSecretName:pot-namedsecretvalues-kapp-appId1-\u0026lt;someGuid1\u0026gt;otherSecrets:internalSecretName:pot-namedsecretvalues-kapp-appId1-\u0026lt;someGuid2\u0026gt;typeSpecificData:fetch:- git:ref:origin/mastersubPath:namedsecretvalues/kapp/resourcesurl:https://github.com/your-path/potter-examplessecretRef:name:potter-examples-accesstemplate:- ytt:{}deploy:- kapp:{}secretRef:\u0026lt;clusterName\u0026gt;.kubeconfigUsage of Named Secrets in kapp deployments In the section typeSpecificData you see an example how named secret values could be used in a kapp deployment. Here the logical secret name potter-examples-access is specified as the name of a secret ref. During the deployment the secret connected with this logical name is used to access a private git repository. You could enter logical secret names everywhere in the type specific data section of a kapp deployment where a secret ref could be used.\nUsage of Named Secrets in helm deployments In helm deployments, the data of every named secret is provided as an additional values file during templating. These template files are applied after the values specified in the secret values section. There is no predefined order in which the data of the different named secrets are applied.\nA named secret could be also referenced in a tarball access as in the following example:\napiVersion:hub.k8s.sap.com/v1kind:ClusterBommetadata:name:pot-namedsecretvalues-authheadernamespace:\u0026lt;some-garden-namespace\u0026gt;spec:applicationConfigs:- configType:helmid:echoservernamedSecretValues:logical-name-1:data:authHeader:\u0026#34;Basic dX...3dvcmQ=\u0026#34;typeSpecificData:tarballAccess:url:\u0026#34;https://yourpath/echo-server-1.0.5.tgz\u0026#34;secretRef:name:logical-name-1installName:echoservernamespace:\u0026lt;some-target-namespace\u0026gt;secretRef:\u0026lt;your-clustername\u0026gt;.kubeconfigWith this a secret containing the authheader data is automatically created and used for accessing the tgz file. This secret it automatically excluded from being merged as an additional values file during helm template.\nUpdate Secret Values To keep the values of a named secret value with logical name X unchanged, there are several possibilities how to specify this in a Cluster-BoM. Either there is no namedSecretValues section at all or it does not contain X. You could also provide the named secret with identical data or no data.\nTo replace the secret values just provide new data.\nnamedSecretValues:X:data:...# new dataIt is not allowed to delete named secrets values by just providing an empty data section to prevent accidental deletions.\nDelete Secret Values To delete the secret values use the delete operation:\nnamedSecretValues:X:operation:delete"
},
{
	"uri": "https://gardener.github.io/potter-docs/controller-docs/docs/special-topics/named-secrets/",
	"title": "Named Secrets",
	"tags": [],
	"description": "",
	"content": "Basics and Example Named secret values allow to specify secret values in a Cluster-BoM which are automatically moved to secrets before the Cluster-BoM is stored or updated. These secrets could be referenced in kapp deployments via their logical names. In helm deployments the secret values stored in the secrets are added as additional values files for templating.\nThe following example shows a Cluster-BoM with a section namedSecretValues with two named secret values potter-examples-access and otherSecrets. Make sure that the values of the top level map entries under data are strings.\napiVersion:hub.k8s.sap.com/v1kind:ClusterBommetadata:name:pot-namedsecretvalues-kappnamespace:\u0026lt;some-garden-namespace\u0026gt;spec:applicationConfigs:- configType:kappid:appId1namedSecretValues:potter-examples-access:# logical secret namedata:username:YOUR_USER # moved to secretpassword:YOUR_ACCESS_TOKEN # moved to secretotherSecrets:data:otherSecret1:|secret1: s11 secret2: s21otherSecret2:|secret3: user: users3 password: pw3typeSpecificData:fetch:- git:ref:origin/mastersubPath:namedsecretvalues/kapp/resourcesurl:https://github.com/your-path/potter-examplessecretRef:name:potter-examples-accesstemplate:- ytt:{}deploy:- kapp:{}secretRef:\u0026lt;clusterName\u0026gt;.kubeconfigWhen this Cluster-BoM is deployed, the data of potter-examples-access and otherSecrets are stored in two automatically generated secrets in the following format:\napiVersion:v1kind:Secretmetadata:labels:# internal secret name, not the logical secret name name:pot-namedsecretvalues-kapp-appId1-\u0026lt;someGuid1\u0026gt;namespace:\u0026lt;some-garden-namespace\u0026gt;type:Opaquedata:username:YOUR_USER password:YOUR_ACCESS_TOKENapiVersion:v1kind:Secretmetadata:labels:# internal secret name, not the logical secret name name:pot-namedsecretvalues-kapp-appId1-\u0026lt;someGuid2\u0026gt;namespace:\u0026lt;some-garden-namespace\u0026gt;type:Opaquedata:otherSecret1:secret1:s11secret2:s21otherSecret2:secret3:user:users3password:pw3The stored Cluster-BoM itself only contains references to these secrets instead of the data:\napiVersion:hub.k8s.sap.com/v1kind:ClusterBommetadata:name:pot-namedsecretvalues-kappnamespace:\u0026lt;some-garden-namespace\u0026gt;spec:applicationConfigs:- configType:kappid:appId1namedSecretValues:potter-examples-access:internalSecretName:pot-namedsecretvalues-kapp-appId1-\u0026lt;someGuid1\u0026gt;otherSecrets:internalSecretName:pot-namedsecretvalues-kapp-appId1-\u0026lt;someGuid2\u0026gt;typeSpecificData:fetch:- git:ref:origin/mastersubPath:namedsecretvalues/kapp/resourcesurl:https://github.com/your-path/potter-examplessecretRef:name:potter-examples-accesstemplate:- ytt:{}deploy:- kapp:{}secretRef:\u0026lt;clusterName\u0026gt;.kubeconfigUsage of Named Secrets in kapp deployments In the section typeSpecificData you see an example how named secret values could be used in a kapp deployment. Here the logical secret name potter-examples-access is specified as the name of a secret ref. During the deployment the secret connected with this logical name is used to access a private git repository. You could enter logical secret names everywhere in the type specific data section of a kapp deployment where a secret ref could be used.\nUsage of Named Secrets in helm deployments In helm deployments, the data of every named secret is provided as an additional values file during templating. These template files are applied after the values specified in the secret values section. There is no predefined order in which the data of the different named secrets are applied.\nA named secret could be also referenced in a tarball access as in the following example:\napiVersion:hub.k8s.sap.com/v1kind:ClusterBommetadata:name:pot-namedsecretvalues-authheadernamespace:\u0026lt;some-garden-namespace\u0026gt;spec:applicationConfigs:- configType:helmid:echoservernamedSecretValues:logical-name-1:data:authHeader:\u0026#34;Basic dX...3dvcmQ=\u0026#34;typeSpecificData:tarballAccess:url:\u0026#34;https://yourpath/echo-server-1.0.5.tgz\u0026#34;secretRef:name:logical-name-1installName:echoservernamespace:\u0026lt;some-target-namespace\u0026gt;secretRef:\u0026lt;your-clustername\u0026gt;.kubeconfigWith this a secret containing the authheader data is automatically created and used for accessing the tgz file. This secret it automatically excluded from being merged as an additional values file during helm template.\nUpdate Secret Values To keep the values of a named secret value with logical name X unchanged, there are several possibilities how to specify this in a Cluster-BoM. Either there is no namedSecretValues section at all or it does not contain X. You could also provide the named secret with identical data or no data.\nTo replace the secret values just provide new data.\nnamedSecretValues:X:data:...# new dataIt is not allowed to delete named secrets values by just providing an empty data section to prevent accidental deletions.\nDelete Secret Values To delete the secret values use the delete operation:\nnamedSecretValues:X:operation:delete"
},
{
	"uri": "https://gardener.github.io/potter-docs/",
	"title": "Potter",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gardener.github.io/potter-docs/controller-docs-0.112.0/docs/special-topics/resource-ready-requirements/",
	"title": "Ready Requirements",
	"tags": [],
	"description": "",
	"content": "Resource Ready Requirements This feature allows users of ClusterBoMs to specify properties from arbitrary K8s resources on a target cluster, that must match against a defined list of values in order for the \u0026ldquo;Ready\u0026rdquo; condition of the ClusterBoM to be \u0026ldquo;True\u0026rdquo;. E.g. in the following simple example, the Complete condition of the K8s Job my-job is defined as a single resource ready requirement.\napiVersion:hub.k8s.sap.com/v1kind:ClusterBommetadata:name:my-bomnamespace:garden-hubtestspec:applicationConfigs:- configType:helmid:my-appreadyRequirements:resources:- name:my-jobnamespace:my-namespaceapiVersion:batch/v1resource:jobsfieldPath:\u0026#34;{ .status.conditions[?(@.type == \u0026#39;Complete\u0026#39;)].status }\u0026#34;successValues:- value:\u0026#34;True\u0026#34;typeSpecificData:catalogAccess:chartName:my-chartchartVersion:5.0.0repo:incubatorinstallName:my-appnamespace:my-namespacesecretRef:my-cluster.kubeconfigKey Points:\n The list of resource ready requirements for one application config is defined via the property applicationConfigs[].readyRequirements.resources. It takes an arbitrary number of resource ready requirements that each must evaluate to true in order for the ClusterBom to be ready. Each single resource ready requirement has the properties name, namespace, apiVersion, and resource. They define the K8s resource on the target cluster which is used for evaluation. The field resource describes the resource kind as plural (Job \u0026ndash;\u0026gt; jobs, Secret \u0026ndash;\u0026gt; secrets, \u0026hellip;). The variable fieldPath addresses a property of the defined K8s resource which is extracted and used for evaluation. fieldPath therefore uses the JSONPath notation. If the resource itself or the property within the resource can\u0026rsquo;t be found, the \u0026ldquo;Ready\u0026rdquo; condition of the ClusterBoM evaluates to \u0026ldquo;Unknown\u0026rdquo;. successValues defines a list of values that the extracted value must match against. Each item in the list must be an object with the single key value. The value behind this key can be of any valid JSON/YAML type and gets used for comparison. Keep in mind that the value that is extracted via fieldPath and successValues must have the same type in order for the ready requirement to be fulfilled. The following example shows how resource ready requirements could be used on user-defined status fields using different data types.  readyRequirements:resources:- name:foo-1namespace:namespace1apiVersion:foo.com/v1resource:foosfieldPath:\u0026#34;{ .status.overallState }\u0026#34;successValues:- value:\u0026#34;ok\u0026#34;- name:bar-1namespace:namespace1apiVersion:bar.com/v1resource:barsfieldPath:\u0026#34;{ .status.myStateObject }\u0026#34;successValues:- value:prop1:\u0026#34;val1\u0026#34;prop2:42- value:prop1:\u0026#34;val2\u0026#34;prop2:42For the second resource from the above example, the extracted value from the defined resource is compared against each object\n{ \u0026#34;prop1\u0026#34;: \u0026#34;val1\u0026#34;, \u0026#34;prop2\u0026#34;: 42 } and\n{ \u0026#34;prop1\u0026#34;: \u0026#34;val2\u0026#34;, \u0026#34;prop2\u0026#34;: 42 } from successValues. The structure of the objects can be arbitrary. The keys and values of the extracted object and the \u0026ldquo;success\u0026rdquo; object must match in order for the ready requirement to be fulfilled.\n"
},
{
	"uri": "https://gardener.github.io/potter-docs/controller-docs-0.113.0/docs/special-topics/resource-ready-requirements/",
	"title": "Ready Requirements",
	"tags": [],
	"description": "",
	"content": "Resource Ready Requirements This feature allows users of ClusterBoMs to specify properties from arbitrary K8s resources on a target cluster, that must match against a defined list of values in order for the \u0026ldquo;Ready\u0026rdquo; condition of the ClusterBoM to be \u0026ldquo;True\u0026rdquo;. E.g. in the following simple example, the Complete condition of the K8s Job my-job is defined as a single resource ready requirement.\napiVersion:hub.k8s.sap.com/v1kind:ClusterBommetadata:name:my-bomnamespace:garden-hubtestspec:applicationConfigs:- configType:helmid:my-appreadyRequirements:resources:- name:my-jobnamespace:my-namespaceapiVersion:batch/v1resource:jobsfieldPath:\u0026#34;{ .status.conditions[?(@.type == \u0026#39;Complete\u0026#39;)].status }\u0026#34;successValues:- value:\u0026#34;True\u0026#34;typeSpecificData:catalogAccess:chartName:my-chartchartVersion:5.0.0repo:incubatorinstallName:my-appnamespace:my-namespacesecretRef:my-cluster.kubeconfigKey Points:\n The list of resource ready requirements for one application config is defined via the property applicationConfigs[].readyRequirements.resources. It takes an arbitrary number of resource ready requirements that each must evaluate to true in order for the ClusterBom to be ready. Each single resource ready requirement has the properties name, namespace, apiVersion, and resource. They define the K8s resource on the target cluster which is used for evaluation. The field resource describes the resource kind as plural (Job \u0026ndash;\u0026gt; jobs, Secret \u0026ndash;\u0026gt; secrets, \u0026hellip;). The variable fieldPath addresses a property of the defined K8s resource which is extracted and used for evaluation. fieldPath therefore uses the JSONPath notation. If the resource itself or the property within the resource can\u0026rsquo;t be found, the \u0026ldquo;Ready\u0026rdquo; condition of the ClusterBoM evaluates to \u0026ldquo;Unknown\u0026rdquo;. successValues defines a list of values that the extracted value must match against. Each item in the list must be an object with the single key value. The value behind this key can be of any valid JSON/YAML type and gets used for comparison. Keep in mind that the value that is extracted via fieldPath and successValues must have the same type in order for the ready requirement to be fulfilled. The following example shows how resource ready requirements could be used on user-defined status fields using different data types.  readyRequirements:resources:- name:foo-1namespace:namespace1apiVersion:foo.com/v1resource:foosfieldPath:\u0026#34;{ .status.overallState }\u0026#34;successValues:- value:\u0026#34;ok\u0026#34;- name:bar-1namespace:namespace1apiVersion:bar.com/v1resource:barsfieldPath:\u0026#34;{ .status.myStateObject }\u0026#34;successValues:- value:prop1:\u0026#34;val1\u0026#34;prop2:42- value:prop1:\u0026#34;val2\u0026#34;prop2:42For the second resource from the above example, the extracted value from the defined resource is compared against each object\n{ \u0026#34;prop1\u0026#34;: \u0026#34;val1\u0026#34;, \u0026#34;prop2\u0026#34;: 42 } and\n{ \u0026#34;prop1\u0026#34;: \u0026#34;val2\u0026#34;, \u0026#34;prop2\u0026#34;: 42 } from successValues. The structure of the objects can be arbitrary. The keys and values of the extracted object and the \u0026ldquo;success\u0026rdquo; object must match in order for the ready requirement to be fulfilled.\n"
},
{
	"uri": "https://gardener.github.io/potter-docs/controller-docs/docs/special-topics/resource-ready-requirements/",
	"title": "Ready Requirements",
	"tags": [],
	"description": "",
	"content": "Resource Ready Requirements This feature allows users of ClusterBoMs to specify properties from arbitrary K8s resources on a target cluster, that must match against a defined list of values in order for the \u0026ldquo;Ready\u0026rdquo; condition of the ClusterBoM to be \u0026ldquo;True\u0026rdquo;. E.g. in the following simple example, the Complete condition of the K8s Job my-job is defined as a single resource ready requirement.\napiVersion:hub.k8s.sap.com/v1kind:ClusterBommetadata:name:my-bomnamespace:garden-hubtestspec:applicationConfigs:- configType:helmid:my-appreadyRequirements:resources:- name:my-jobnamespace:my-namespaceapiVersion:batch/v1resource:jobsfieldPath:\u0026#34;{ .status.conditions[?(@.type == \u0026#39;Complete\u0026#39;)].status }\u0026#34;successValues:- value:\u0026#34;True\u0026#34;typeSpecificData:catalogAccess:chartName:my-chartchartVersion:5.0.0repo:incubatorinstallName:my-appnamespace:my-namespacesecretRef:my-cluster.kubeconfigKey Points:\n The list of resource ready requirements for one application config is defined via the property applicationConfigs[].readyRequirements.resources. It takes an arbitrary number of resource ready requirements that each must evaluate to true in order for the ClusterBom to be ready. Each single resource ready requirement has the properties name, namespace, apiVersion, and resource. They define the K8s resource on the target cluster which is used for evaluation. The field resource describes the resource kind as plural (Job \u0026ndash;\u0026gt; jobs, Secret \u0026ndash;\u0026gt; secrets, \u0026hellip;). The variable fieldPath addresses a property of the defined K8s resource which is extracted and used for evaluation. fieldPath therefore uses the JSONPath notation. If the resource itself or the property within the resource can\u0026rsquo;t be found, the \u0026ldquo;Ready\u0026rdquo; condition of the ClusterBoM evaluates to \u0026ldquo;Unknown\u0026rdquo;. successValues defines a list of values that the extracted value must match against. Each item in the list must be an object with the single key value. The value behind this key can be of any valid JSON/YAML type and gets used for comparison. Keep in mind that the value that is extracted via fieldPath and successValues must have the same type in order for the ready requirement to be fulfilled. The following example shows how resource ready requirements could be used on user-defined status fields using different data types.  readyRequirements:resources:- name:foo-1namespace:namespace1apiVersion:foo.com/v1resource:foosfieldPath:\u0026#34;{ .status.overallState }\u0026#34;successValues:- value:\u0026#34;ok\u0026#34;- name:bar-1namespace:namespace1apiVersion:bar.com/v1resource:barsfieldPath:\u0026#34;{ .status.myStateObject }\u0026#34;successValues:- value:prop1:\u0026#34;val1\u0026#34;prop2:42- value:prop1:\u0026#34;val2\u0026#34;prop2:42For the second resource from the above example, the extracted value from the defined resource is compared against each object\n{ \u0026#34;prop1\u0026#34;: \u0026#34;val1\u0026#34;, \u0026#34;prop2\u0026#34;: 42 } and\n{ \u0026#34;prop1\u0026#34;: \u0026#34;val2\u0026#34;, \u0026#34;prop2\u0026#34;: 42 } from successValues. The structure of the objects can be arbitrary. The keys and values of the extracted object and the \u0026ldquo;success\u0026rdquo; object must match in order for the ready requirement to be fulfilled.\n"
},
{
	"uri": "https://gardener.github.io/potter-docs/controller-docs-0.112.0/docs/special-topics/secret-handling/",
	"title": "Secret Handling",
	"tags": [],
	"description": "",
	"content": "Please note: The feature described here is deprecated and will be removed end of April 2021. For a more flexible way to create and use secrets see Named Secrets. This section is only relevant for Helm deployments.\nIn kubernetes, credentials and other secret data should only be stored in secrets, but not in other kinds of resources like clusterboms. Therefore the Application Hub provides a way to handle secret data during deployments.\nYou can include secret data in a section secretValues in every application config of the Cluster-BoM.\nspec:applicationConfigs:- id:...configType:...typeSpecificData:...values:...secretValues:data:credentials:- username:\u0026lt;some name\u0026gt;password:\u0026lt;some password\u0026gt;Before the Cluster-BoM is saved, the secret data are moved into a secret resource, one secret for every application config with secret values. Only a reference to the secret remains in the clusterbom. If you fetch the Cluster-BoM using kubectl get clusterbom ... you get the following:\n...secretValues:internalSecretName:\u0026lt;secret name\u0026gt;...The secret is stored in the same cluster (garden cluster) and namespace as the Cluster-BoM.\nHub managed secrets cannot be modified or deleted by the user. The user can modify secret values only by changing the Cluster-BoM. If the user changes secret values, a new secret is created and the internal secret name in the clusterbom is replaced by the new secret name.\nDuring the helm deployment operation, the secret values are merged into the (normal) values section. Thereby keys are added and values replaced as in the example below. Given the following part of a Cluster-BoM:\nspec:applicationConfigs:- id:...configType:...typeSpecificData:...values:test:key1:val11key2:- val21- val22- val23key4:val41secretValues:data:test:key1:val12key2:- val24- val25key3:val31This results in the following merged values file provided to the helm deploy operation:\ntest:key1:val12key2:- val24- val25key3:val31key4:val41Update Secret Values To keep the secret values unchanged, there are several possibilities how to specify this in the Cluster-BoM. Either there is no secretValues section at all or you are using one of the following alternatives:\nsecretValues:operation:keepor\nsecretValues:operation:replacedata:...# same data as beforeor\nsecretValues:internalSecretName:...# same secret name as beforeTo replace the secret values use the replace operation:\nsecretValues:operation:replacedata:...# new dataIt is also possible to just provide new data without any operation to replace the secret values. It is not allowed to delete the secrets values this way by just providing an empty data section.\nsecretValues:data:...# new dataDelete Secret Values To delete the secret values use the delete operation:\nsecretValues:operation:delete"
},
{
	"uri": "https://gardener.github.io/potter-docs/controller-docs-0.113.0/docs/special-topics/secret-handling/",
	"title": "Secret Handling",
	"tags": [],
	"description": "",
	"content": "Please note: The feature described here is deprecated and will be removed end of April 2021. For a more flexible way to create and use secrets see Named Secrets. This section is only relevant for Helm deployments.\nIn kubernetes, credentials and other secret data should only be stored in secrets, but not in other kinds of resources like clusterboms. Therefore the Application Hub provides a way to handle secret data during deployments.\nYou can include secret data in a section secretValues in every application config of the Cluster-BoM.\nspec:applicationConfigs:- id:...configType:...typeSpecificData:...values:...secretValues:data:credentials:- username:\u0026lt;some name\u0026gt;password:\u0026lt;some password\u0026gt;Before the Cluster-BoM is saved, the secret data are moved into a secret resource, one secret for every application config with secret values. Only a reference to the secret remains in the clusterbom. If you fetch the Cluster-BoM using kubectl get clusterbom ... you get the following:\n...secretValues:internalSecretName:\u0026lt;secret name\u0026gt;...The secret is stored in the same cluster (garden cluster) and namespace as the Cluster-BoM.\nHub managed secrets cannot be modified or deleted by the user. The user can modify secret values only by changing the Cluster-BoM. If the user changes secret values, a new secret is created and the internal secret name in the clusterbom is replaced by the new secret name.\nDuring the helm deployment operation, the secret values are merged into the (normal) values section. Thereby keys are added and values replaced as in the example below. Given the following part of a Cluster-BoM:\nspec:applicationConfigs:- id:...configType:...typeSpecificData:...values:test:key1:val11key2:- val21- val22- val23key4:val41secretValues:data:test:key1:val12key2:- val24- val25key3:val31This results in the following merged values file provided to the helm deploy operation:\ntest:key1:val12key2:- val24- val25key3:val31key4:val41Update Secret Values To keep the secret values unchanged, there are several possibilities how to specify this in the Cluster-BoM. Either there is no secretValues section at all or you are using one of the following alternatives:\nsecretValues:operation:keepor\nsecretValues:operation:replacedata:...# same data as beforeor\nsecretValues:internalSecretName:...# same secret name as beforeTo replace the secret values use the replace operation:\nsecretValues:operation:replacedata:...# new dataIt is also possible to just provide new data without any operation to replace the secret values. It is not allowed to delete the secrets values this way by just providing an empty data section.\nsecretValues:data:...# new dataDelete Secret Values To delete the secret values use the delete operation:\nsecretValues:operation:delete"
},
{
	"uri": "https://gardener.github.io/potter-docs/controller-docs/docs/special-topics/secret-handling/",
	"title": "Secret Handling",
	"tags": [],
	"description": "",
	"content": "Please note: The feature described here is deprecated and will be removed end of April 2021. For a more flexible way to create and use secrets see Named Secrets. This section is only relevant for Helm deployments.\nIn kubernetes, credentials and other secret data should only be stored in secrets, but not in other kinds of resources like clusterboms. Therefore the Application Hub provides a way to handle secret data during deployments.\nYou can include secret data in a section secretValues in every application config of the Cluster-BoM.\nspec:applicationConfigs:- id:...configType:...typeSpecificData:...values:...secretValues:data:credentials:- username:\u0026lt;some name\u0026gt;password:\u0026lt;some password\u0026gt;Before the Cluster-BoM is saved, the secret data are moved into a secret resource, one secret for every application config with secret values. Only a reference to the secret remains in the clusterbom. If you fetch the Cluster-BoM using kubectl get clusterbom ... you get the following:\n...secretValues:internalSecretName:\u0026lt;secret name\u0026gt;...The secret is stored in the same cluster (garden cluster) and namespace as the Cluster-BoM.\nHub managed secrets cannot be modified or deleted by the user. The user can modify secret values only by changing the Cluster-BoM. If the user changes secret values, a new secret is created and the internal secret name in the clusterbom is replaced by the new secret name.\nDuring the helm deployment operation, the secret values are merged into the (normal) values section. Thereby keys are added and values replaced as in the example below. Given the following part of a Cluster-BoM:\nspec:applicationConfigs:- id:...configType:...typeSpecificData:...values:test:key1:val11key2:- val21- val22- val23key4:val41secretValues:data:test:key1:val12key2:- val24- val25key3:val31This results in the following merged values file provided to the helm deploy operation:\ntest:key1:val12key2:- val24- val25key3:val31key4:val41Update Secret Values To keep the secret values unchanged, there are several possibilities how to specify this in the Cluster-BoM. Either there is no secretValues section at all or you are using one of the following alternatives:\nsecretValues:operation:keepor\nsecretValues:operation:replacedata:...# same data as beforeor\nsecretValues:internalSecretName:...# same secret name as beforeTo replace the secret values use the replace operation:\nsecretValues:operation:replacedata:...# new dataIt is also possible to just provide new data without any operation to replace the secret values. It is not allowed to delete the secrets values this way by just providing an empty data section.\nsecretValues:data:...# new dataDelete Secret Values To delete the secret values use the delete operation:\nsecretValues:operation:delete"
},
{
	"uri": "https://gardener.github.io/potter-docs/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://gardener.github.io/potter-docs/hub-docs-0.112.0/docs/ui/",
	"title": "User Interface",
	"tags": [],
	"description": "",
	"content": "The Potter Hub UI can be opened in two different modes, with or without a specific target cluster in the URL.\nUsing the Potter Hub UI with a target cluster To manually deploy Helm charts into a cluster, the Potter Hub provides a UI. To access it on the Gardener Dashboard, choose [YOUR-PROJECT] \u0026gt; CLUSTERS \u0026gt; External Tools \u0026gt; K8s Applications and Service Hub*.\nThe UI is divided into three main sections called Applications, Cluster BoMs, and Catalog:\n  The Applications section displays the deployments that are currently available in the Cluster. Note that this view depends on the Namespace selection, which is located in the top-right corner of the UI. The default selection is All Namespaces. Deployments that are managed via Cluster BoMs are marked with a special orange badge. By clicking on this badge, it is possible to directly jump to the corresponding Cluster BoM in the UI.\n  The Cluster BoMs section displays the list of all Cluster BoMs that currently exist for the selected cluster. By clicking on a single Cluster BoM in the list, the Cluster BoM details view is opening. In this view, the detailed state of the Cluster BoM is displayed and upgraded live via WebSockets. Users can also manually trigger a reconcile or download the Cluster BoM YAML via the UI.\n  The Catalog view displays all Helm charts of all Repositories connected to the Potter Hub in a landscape. To deploy a Helm chart, choose the respective card within the catalog, and then Deploy in the upper-right corner. A view is displayed, showing the values that are passed to the Helm chart during deployment. The values can be edited in this view. After clicking Submit, the Helm deployment starts. The Status View displays the current state of the deployment.\n  Note that deployments triggered in the UI are directly deployed to the Cluster, and therefore aren’t handled using Cluster-BoMs or added to an existing Cluster-BoM. This behaviour will change in the very near future.\nUsing the Potter Hub UI without a target cluster If you open the Potter Hub UI without a target cluster in the URL, it only shows the Hub Catalog. This allows you to browse all Helm charts of all repositories connected to the Potter Hub, without being in the context of a target cluster. In this mode, you cannot perform any actions that depend on a target cluster, like deploying/updating/deleting applications. The Hub Catalog can be accessed via \u0026lt;hub-base-url\u0026gt;/#/catalog.\n"
},
{
	"uri": "https://gardener.github.io/potter-docs/hub-docs-0.113.0/docs/ui/",
	"title": "User Interface",
	"tags": [],
	"description": "",
	"content": "The Potter Hub UI can be opened in two different modes, with or without a specific target cluster in the URL.\nUsing the Potter Hub UI with a target cluster To manually deploy Helm charts into a cluster, the Potter Hub provides a UI. To access it on the Gardener Dashboard, choose [YOUR-PROJECT] \u0026gt; CLUSTERS \u0026gt; External Tools \u0026gt; K8s Applications and Service Hub*.\nThe UI is divided into three main sections called Applications, Cluster BoMs, and Catalog:\n  The Applications section displays the deployments that are currently available in the Cluster. Note that this view depends on the Namespace selection, which is located in the top-right corner of the UI. The default selection is All Namespaces. Deployments that are managed via Cluster BoMs are marked with a special orange badge. By clicking on this badge, it is possible to directly jump to the corresponding Cluster BoM in the UI.\n  The Cluster BoMs section displays the list of all Cluster BoMs that currently exist for the selected cluster. By clicking on a single Cluster BoM in the list, the Cluster BoM details view is opening. In this view, the detailed state of the Cluster BoM is displayed and upgraded live via WebSockets. Users can also manually trigger a reconcile or download the Cluster BoM YAML via the UI.\n  The Catalog view displays all Helm charts of all Repositories connected to the Potter Hub in a landscape. To deploy a Helm chart, choose the respective card within the catalog, and then Deploy in the upper-right corner. A view is displayed, showing the values that are passed to the Helm chart during deployment. The values can be edited in this view. After clicking Submit, the Helm deployment starts. The Status View displays the current state of the deployment.\n  Note that deployments triggered in the UI are directly deployed to the Cluster, and therefore aren’t handled using Cluster-BoMs or added to an existing Cluster-BoM. This behaviour will change in the very near future.\nUsing the Potter Hub UI without a target cluster If you open the Potter Hub UI without a target cluster in the URL, it only shows the Hub Catalog. This allows you to browse all Helm charts of all repositories connected to the Potter Hub, without being in the context of a target cluster. In this mode, you cannot perform any actions that depend on a target cluster, like deploying/updating/deleting applications. The Hub Catalog can be accessed via \u0026lt;hub-base-url\u0026gt;/#/catalog.\n"
},
{
	"uri": "https://gardener.github.io/potter-docs/hub-docs/docs/ui/",
	"title": "User Interface",
	"tags": [],
	"description": "",
	"content": "The Potter Hub UI can be opened in two different modes, with or without a specific target cluster in the URL.\nUsing the Potter Hub UI with a target cluster To manually deploy Helm charts into a cluster, the Potter Hub provides a UI. To access it on the Gardener Dashboard, choose [YOUR-PROJECT] \u0026gt; CLUSTERS \u0026gt; External Tools \u0026gt; K8s Applications and Service Hub*.\nThe UI is divided into three main sections called Applications, Cluster BoMs, and Catalog:\n  The Applications section displays the deployments that are currently available in the Cluster. Note that this view depends on the Namespace selection, which is located in the top-right corner of the UI. The default selection is All Namespaces. Deployments that are managed via Cluster BoMs are marked with a special orange badge. By clicking on this badge, it is possible to directly jump to the corresponding Cluster BoM in the UI.\n  The Cluster BoMs section displays the list of all Cluster BoMs that currently exist for the selected cluster. By clicking on a single Cluster BoM in the list, the Cluster BoM details view is opening. In this view, the detailed state of the Cluster BoM is displayed and upgraded live via WebSockets. Users can also manually trigger a reconcile or download the Cluster BoM YAML via the UI.\n  The Catalog view displays all Helm charts of all Repositories connected to the Potter Hub in a landscape. To deploy a Helm chart, choose the respective card within the catalog, and then Deploy in the upper-right corner. A view is displayed, showing the values that are passed to the Helm chart during deployment. The values can be edited in this view. After clicking Submit, the Helm deployment starts. The Status View displays the current state of the deployment.\n  Note that deployments triggered in the UI are directly deployed to the Cluster, and therefore aren’t handled using Cluster-BoMs or added to an existing Cluster-BoM. This behaviour will change in the very near future.\nUsing the Potter Hub UI without a target cluster If you open the Potter Hub UI without a target cluster in the URL, it only shows the Hub Catalog. This allows you to browse all Helm charts of all repositories connected to the Potter Hub, without being in the context of a target cluster. In this mode, you cannot perform any actions that depend on a target cluster, like deploying/updating/deleting applications. The Hub Catalog can be accessed via \u0026lt;hub-base-url\u0026gt;/#/catalog.\n"
}]